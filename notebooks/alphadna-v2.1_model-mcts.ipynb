{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from tqdm import tqdm\n",
    "import copy, math\n",
    "\n",
    "from cremerl import utils, model_zoo, shuffle\n",
    "\n",
    "import shuffle_test\n",
    "\n",
    "#import gymnasium as gym\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set the logging level to WARNING\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = 'DeepSTARR'\n",
    "\n",
    "# load data\n",
    "data_path = '../../data/'\n",
    "filepath = os.path.join(data_path, expt_name+'_data.h5')\n",
    "data_module = utils.H5DataModule(filepath, batch_size=100, lower_case=False, transpose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyu/.conda/envs/tf_2/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "2023-08-11 14:50:27.930989: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-11 14:50:28.445878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/yiyu/.conda/envs/tf_2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a439c95b195470585e74fc1bd0ea915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepstarr2 = model_zoo.deepstarr(2)\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer_dict = utils.configure_optimizer(deepstarr2, lr=0.001, weight_decay=1e-6, decay_factor=0.1, patience=5, monitor='val_loss')\n",
    "standard_cnn = model_zoo.DeepSTARR(deepstarr2,\n",
    "                                  criterion=loss,\n",
    "                                  optimizer=optimizer_dict)\n",
    "\n",
    "# load checkpoint for model with best validation performance\n",
    "standard_cnn = utils.load_model_from_checkpoint(standard_cnn, 'DeepSTARR_standard.ckpt')\n",
    "\n",
    "# evaluate best model\n",
    "pred = utils.get_predictions(standard_cnn, data_module.x_test[np.newaxis,100], batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swap_greedy(x, x_mut, tile_ranges):\n",
    "    ori = x.copy()\n",
    "    mut = x_mut.copy()\n",
    "    for tile_range in tile_ranges:\n",
    "        ori[:, tile_range[0]:tile_range[1]] = x_mut[:, tile_range[0]:tile_range[1]]\n",
    "        mut[:, tile_range[0]:tile_range[1]] = x[:, tile_range[0]:tile_range[1]]\n",
    "\n",
    "    return ori, mut\n",
    "\n",
    "def get_score(pred):\n",
    "    score1 = pred[0] - pred[2]\n",
    "    score2 = pred[3] - pred[1]\n",
    "    return (score1+score2)[0], score1+score2\n",
    "\n",
    "def generate_tile_ranges(sequence_length, window_size, stride):\n",
    "    ranges = []\n",
    "    start = np.arange(0, sequence_length - window_size + stride, stride)\n",
    "\n",
    "    for s in start:\n",
    "        e = min(s + window_size, sequence_length)\n",
    "        ranges.append([s, e])\n",
    "\n",
    "    if start[-1] + window_size - stride < sequence_length:  # Adjust the last range\n",
    "        ranges[-1][1] = sequence_length\n",
    "\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, tile_range, tile_ranges_ori, trials):\n",
    "    test_batch = []\n",
    "    for i in range(trials):\n",
    "        test_batch.append(x)\n",
    "        x_mut = shuffle.dinuc_shuffle(x.copy())\n",
    "        test_batch.append(x_mut)\n",
    "\n",
    "        ori = x.copy()\n",
    "        mut = x_mut.copy()\n",
    "        \n",
    "        ori, mut = get_swap_greedy(ori, mut, tile_ranges_ori)\n",
    "        \n",
    "        ori[:, tile_range[0]:tile_range[1]] = x_mut[:, tile_range[0]:tile_range[1]]\n",
    "        mut[:, tile_range[0]:tile_range[1]] = x[:, tile_range[0]:tile_range[1]]\n",
    "        \n",
    "        test_batch.append(ori)\n",
    "        test_batch.append(mut)\n",
    "\n",
    "    #print(np.array(test_batch).shape)\n",
    "    return np.array(test_batch)\n",
    "\n",
    "\n",
    "def get_batch_score(pred, trials):\n",
    "\n",
    "    score = []\n",
    "    score_sep = []\n",
    "    for i in range(0, pred.shape[0], 2):\n",
    "        # print(f\"Viewing number {i}\")\n",
    "        score1 = pred[0] - pred[i]\n",
    "        score2 = pred[i+1] - pred[1]\n",
    "        score.append((np.sum((score1, score2)[0])).tolist()) #np.sum(score1+score2, keepdims=True)\n",
    "        score_sep.append((score1+score2).tolist())\n",
    "        \n",
    "    # print(score)\n",
    "        \n",
    "    final = np.sum(np.array(score), axis=0)/trials\n",
    "\n",
    "    #max_ind = np.argmax(final)\n",
    "    #block_ind = np.argmax(np.array(score)[:, max_ind])\n",
    "    #print(np.array(total_score)[:, max_ind])\n",
    "    total_score_sep = np.sum(np.array(score_sep), axis=0)/trials\n",
    "\n",
    "    #print(np.max(score))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_sequence(one_hot_sequence):\n",
    "    A, L = one_hot_sequence.shape\n",
    "\n",
    "    # Create an all-ones row\n",
    "    ones_row = np.zeros(L)\n",
    "\n",
    "    # Add the all-ones row to the original sequence\n",
    "    new_sequence = np.vstack((one_hot_sequence, ones_row))\n",
    "\n",
    "    return np.array(new_sequence, dtype='float32')\n",
    "\n",
    "def taking_action(sequence_with_ones, tile_range):\n",
    "    start_idx, end_idx = tile_range\n",
    "\n",
    "    # Ensure the start_idx and end_idx are within valid bounds\n",
    "    #if start_idx < 0 or start_idx >= sequence_with_ones.shape[1] or end_idx < 0 or end_idx >= sequence_with_ones.shape[1]:\n",
    "    #    raise ValueError(\"Invalid tile range indices.\")\n",
    "\n",
    "    # Copy the input sequence to avoid modifying the original sequence\n",
    "    modified_sequence = sequence_with_ones.copy()\n",
    "\n",
    "    # Modify the last row within the specified tile range\n",
    "    modified_sequence[-1, start_idx:end_idx] = 1\n",
    "\n",
    "    return np.array(modified_sequence, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_elements(input_list):\n",
    "    input_list = input_list.tolist()\n",
    "    num_columns = 5  # Number of elements to process in each group\n",
    "\n",
    "    # Calculate the number of elements needed to pad the list\n",
    "    padding_length = num_columns - (len(input_list) % num_columns)\n",
    "    last_value = input_list[-1]\n",
    "    padded_list = input_list + [last_value] * padding_length\n",
    "\n",
    "    # Convert the padded list to a NumPy array for efficient operations\n",
    "    input_array = np.array(padded_list)\n",
    "    reshaped_array = input_array.reshape(-1, num_columns)\n",
    "\n",
    "    # Check if each row has the same value (all 0s or all 1s)\n",
    "    row_all_zeros = np.all(reshaped_array == 0, axis=1)\n",
    "    row_all_ones = np.all(reshaped_array == 1, axis=1)\n",
    "\n",
    "    # Replace all 0s with 0 and all 1s with 1 in the result array\n",
    "    output_array = np.where(row_all_zeros, 0, np.where(row_all_ones, 1, reshaped_array[:, 0]))\n",
    "\n",
    "    # Flatten the result array to get the final output list\n",
    "    output_list = output_array.flatten()\n",
    "\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqGame:\n",
    "    def __init__(self, sequence, model_func):\n",
    "        self.seq = sequence\n",
    "        self.ori_seq = sequence.copy()\n",
    "        # self.tile_ranges = generate_tile_ranges(sequence.shape[1], 5, 5)\n",
    "        # self.tile_ranges_done = []\n",
    "        self.levels = 20\n",
    "        self.num_trials = 10\n",
    "        self.action_size = 50\n",
    "        \n",
    "        self.prev_score = -float(\"inf\")\n",
    "        self.current_score = 0\n",
    "        \n",
    "        self.trainer = pl.Trainer(accelerator='gpu', devices='1', logger=None, enable_progress_bar=False)\n",
    "        self.model = model_func\n",
    "        \n",
    "        if self.seq.shape[0]!=5:\n",
    "            self.seq = extend_sequence(self.seq)\n",
    "            self.ori_seq = extend_sequence(self.ori_seq)\n",
    "        \n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        self.seq = self.ori_seq.copy()\n",
    "        self.tile_ranges = generate_tile_ranges(self.seq.shape[1], 5, 5)\n",
    "        self.tile_ranges_done = []\n",
    "        self.current_level = 0\n",
    "        \n",
    "        return self.seq\n",
    "    \n",
    "    \n",
    "    def get_next_state(self, action, tile_ranges, tile_ranges_done):\n",
    "        self.prev_score = self.current_score\n",
    "        # self.current_level += 1\n",
    "        \n",
    "        self.seq = taking_action(self.seq, tile_ranges[action])\n",
    "        \n",
    "        batch = get_batch(self.seq[:4, :], tile_ranges[action], tile_ranges_done, self.num_trials)\n",
    "        dataloader = torch.utils.data.DataLoader(batch, batch_size=100, shuffle=False)\n",
    "        pred = np.concatenate(self.trainer.predict(self.model, dataloaders=dataloader))\n",
    "        \n",
    "        self.current_score = np.tanh(1 * get_batch_score(pred, self.num_trials)) #ADDED TANH\n",
    "        \n",
    "        return self.seq\n",
    "    \n",
    "    def get_valid_moves(self):\n",
    "        return (convert_elements(self.seq[-1, :]) == 0).astype(np.uint8)\n",
    "    \n",
    "    def terminate(self, level, current_score, parent_score): #state\n",
    "        # if self.current_level >= self.levels:\n",
    "        #     return True\n",
    "        # if self.current_score < self.prev_score:\n",
    "        #     return True\n",
    "        \n",
    "        if level >= self.levels:\n",
    "            return True\n",
    "        if current_score < parent_score:\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "    \n",
    "    def set_seq(self, seq):\n",
    "        self.seq = seq\n",
    "    \n",
    "    def get_seq(self):\n",
    "        return self.seq.copy()\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, env, args, parent=None, action_taken=None, prior=0):\n",
    "        self.env = env\n",
    "        self.args = args\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "        \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior #could use np.sqrt or np.log\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_env = copy.deepcopy(self.env)\n",
    "                child_state = child_env.get_next_state(action)\n",
    "                \n",
    "                child = Node(child_env, self.args, self, action, prob)\n",
    "                self.children.append(child)\n",
    "        \n",
    "        return child\n",
    "    \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.env.current_score\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, env, args, model):\n",
    "        self.env = env\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "    \n",
    "    #@torch.no_grad()\n",
    "    def search(self):\n",
    "        root = Node(self.env, self.args)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            print(f\"Conducting search no. {search}\")\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "            \n",
    "            value = self.env.current_score\n",
    "            is_terminal = self.env.terminate()\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(torch.tensor(node.env.seq).unsqueeze(0))\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().detach().numpy()\n",
    "                valid_moves = node.env.get_valid_moves()\n",
    "                policy = policy * valid_moves / np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                node = node.expand(policy)\n",
    "\n",
    "            node.backpropagate(value)\n",
    "        \n",
    "        action_probs = np.zeros(self.env.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, action, state, done, reward, mcts, parent=None):\n",
    "        self.env = parent.env\n",
    "        self.action = action\n",
    "        \n",
    "        self.is_expanded = False\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        \n",
    "        self.action_space_size = self.env.action_size\n",
    "        self.child_total_value = np.zeros(\n",
    "            [self.action_space_size], dtype=np.float32\n",
    "        ) # Q\n",
    "        self.child_priors = np.zeros([self.action_space_size], dtype=np.float32) # P\n",
    "        self.child_number_visits = np.zeros(\n",
    "            [self.action_space_size], dtype=np.float32\n",
    "        ) # N\n",
    "        self.valid_actions = self.env.get_valid_moves().astype(np.bool_)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "        self.state = state\n",
    "        \n",
    "        self.mcts = mcts\n",
    "    \n",
    "    @property\n",
    "    def number_visits(self):\n",
    "        return self.parent.child_number_visits[self.action]\n",
    "    \n",
    "    @number_visits.setter\n",
    "    def number_visits(self, value):\n",
    "        self.parent.child_number_visits[self.action] = value\n",
    "        \n",
    "    @property\n",
    "    def total_value(self):\n",
    "        return self.parent.child_total_value[self.action]\n",
    "    \n",
    "    @total_value.setter\n",
    "    def total_value(self, value):\n",
    "        self.parent.child_total_value[self.action] = value\n",
    "        \n",
    "    def child_Q(self):\n",
    "        return self.child_total_value / (1 + self.child_number_visits)\n",
    "    \n",
    "    def child_U(self):\n",
    "        return (\n",
    "            math.sqrt(self.number_visits)\n",
    "            * self.child_priors\n",
    "            / (1 + self.child_number_visits)\n",
    "        )\n",
    "    \n",
    "    def best_action(self):\n",
    "        child_score = self.child_Q() + self.mcts.c_puct * self.child_U()\n",
    "        masked_child_score = child_score\n",
    "        masked_child_score[~self.valid_actions] = -np.inf\n",
    "        return np.argmax(masked_child_score)\n",
    "    \n",
    "    def select(self):\n",
    "        current_node = self\n",
    "        while current_node.is_expanded:\n",
    "            best_action = current_node.best_action()\n",
    "            current_node = current_node.get_child(best_action)\n",
    "        return current_node\n",
    "    \n",
    "    def expand(self, child_priors):\n",
    "        self.is_expanded = True\n",
    "        self.child_priors = child_priors\n",
    "    \n",
    "    def get_child(self, action):\n",
    "        if action not in self.children:\n",
    "            self.env.set_seq(self.state)\n",
    "            next_state = self.env.get_next_state(action)\n",
    "            reward = self.env.get_score()\n",
    "            terminated = self.env.terminate()\n",
    "            self.children[action] = Node(\n",
    "                state=next_state, \n",
    "                action=action, \n",
    "                parent=self, \n",
    "                reward=reward,\n",
    "                done=terminated,\n",
    "                mcts=self.mcts\n",
    "            )\n",
    "        return self.children[action]\n",
    "    \n",
    "    def backup(self, value):\n",
    "        current = self\n",
    "        while current.parent is not None:\n",
    "            current.number_visits += 1\n",
    "            current.total_value += value\n",
    "            current = current.parent\n",
    "\n",
    "class RootParentNode:\n",
    "    def __init__(self, env):\n",
    "        self.parent = None\n",
    "        self.child_total_value = collections.defaultdict(float)\n",
    "        self.child_number_visits = collections.defaultdict(float)\n",
    "        self.env = env\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, model, mcts_param):\n",
    "        self.model = model\n",
    "        self.temperature = mcts_param[\"temperature\"]\n",
    "        self.dir_epsilon = mcts_param[\"dirichlet_epsilon\"]\n",
    "        self.dir_noise = mcts_param[\"dirichlet_noise\"]\n",
    "        self.num_sims = mcts_param[\"num_simulations\"]\n",
    "        self.exploit = mcts_param[\"argmax_tree_policy\"]\n",
    "        self.add_dirichlet_noise = mcts_param[\"add_dirichlet_noise\"]\n",
    "        self.c_puct = mcts_param[\"puct_coefficient\"]\n",
    "    \n",
    "    def compute_action(self, node):\n",
    "        for _ in range(self.num_sims):\n",
    "            leaf = node.select()\n",
    "            if leaf.done:\n",
    "                value = leaf.reward\n",
    "            else:\n",
    "                child_priors, value = self.model(torch.tensor(leaf.state).unsqueeze(0))\n",
    "                child_priors = torch.softmax(child_priors, axis=1).squeeze(0).cpu().detach().numpy()\n",
    "                if self.add_dirichlet_noise:\n",
    "                    child_priors = (1 - self.dir_epsilon) * child_priors\n",
    "                    child_priors += self.dir_epsilon * np.random.dirichlet(\n",
    "                        [self.dir_noise] * child_priors.size\n",
    "                    )\n",
    "                \n",
    "                leaf.expand(child_priors)\n",
    "            leaf.backup(value)\n",
    "            \n",
    "        tree_policy = node.child_number_visits / node.number_visits\n",
    "        tree_policy = tree_policy / np.max(tree_policy)\n",
    "        tree_policy = np.power(tree_policy, self.temperature)\n",
    "        tree_policy = tree_policy / np.sum(tree_policy)\n",
    "        if self.exploit:\n",
    "            action = np.argmax(tree_policy)\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(node.action_space_size), p=tree_policy)\n",
    "        return tree_policy, action, node.children[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, action, state, done, reward, mcts, level, tile_ranges, tile_ranges_done, parent=None):\n",
    "        self.env = parent.env\n",
    "        self.action = action\n",
    "        \n",
    "        self.is_expanded = False\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        \n",
    "        self.action_space_size = self.env.action_size\n",
    "        self.child_total_value = np.zeros(\n",
    "            [self.action_space_size], dtype=np.float32\n",
    "        ) # Q\n",
    "        self.child_priors = np.zeros([self.action_space_size], dtype=np.float32) # P\n",
    "        self.child_number_visits = np.zeros(\n",
    "            [self.action_space_size], dtype=np.float32\n",
    "        ) # N\n",
    "        self.valid_actions = (convert_elements(state[-1, :]) == 0).astype(np.uint8)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "        self.state = state\n",
    "        self.level = level\n",
    "        \n",
    "        self.tile_ranges = tile_ranges\n",
    "        self.tile_ranges_done = tile_ranges_done\n",
    "        \n",
    "        self.mcts = mcts\n",
    "    \n",
    "    @property\n",
    "    def number_visits(self):\n",
    "        return self.parent.child_number_visits[self.action]\n",
    "    \n",
    "    @number_visits.setter\n",
    "    def number_visits(self, value):\n",
    "        self.parent.child_number_visits[self.action] = value\n",
    "        \n",
    "    @property\n",
    "    def total_value(self):\n",
    "        return self.parent.child_total_value[self.action]\n",
    "    \n",
    "    @total_value.setter\n",
    "    def total_value(self, value):\n",
    "        self.parent.child_total_value[self.action] = value\n",
    "        \n",
    "    def child_Q(self):\n",
    "        return self.child_total_value / (1 + self.child_number_visits)\n",
    "    \n",
    "    def child_U(self):\n",
    "        return (\n",
    "            math.sqrt(self.number_visits)\n",
    "            * self.child_priors\n",
    "            / (1 + self.child_number_visits)\n",
    "        )\n",
    "    \n",
    "    def best_action(self):\n",
    "        child_score = self.child_Q() + self.mcts.c_puct * self.child_U()\n",
    "        masked_child_score = child_score\n",
    "        # masked_child_score[~self.valid_actions] = -np.inf\n",
    "        masked_child_score = masked_child_score * self.valid_actions\n",
    "        return np.argmax(masked_child_score)\n",
    "    \n",
    "    def select(self):\n",
    "        current_node = self\n",
    "        while current_node.is_expanded:\n",
    "            best_action = current_node.best_action()\n",
    "            current_node = current_node.get_child(best_action)\n",
    "        return current_node\n",
    "    \n",
    "    def expand(self, child_priors):\n",
    "        self.is_expanded = True\n",
    "        self.child_priors = child_priors\n",
    "    \n",
    "    def get_child(self, action):\n",
    "        if action not in self.children:\n",
    "            \n",
    "            \n",
    "            self.env.set_seq(self.state)\n",
    "            next_state = self.env.get_next_state(action, self.tile_ranges, self.tile_ranges_done)\n",
    "            # self.tile_ranges_done.append(self.tile_ranges.pop(action))\n",
    "            self.tile_ranges_done.append(self.tile_ranges[action])\n",
    "            self.tile_ranges[action] = [0, 0]\n",
    "            # swap tile_ranges\n",
    "            reward = self.env.get_score()\n",
    "            terminated = self.env.terminate(self.level, reward, self.parent.reward)\n",
    "            self.children[action] = Node(\n",
    "                state=next_state, \n",
    "                action=action, \n",
    "                parent=self, \n",
    "                reward=reward,\n",
    "                done=terminated,\n",
    "                mcts=self.mcts, \n",
    "                level=self.level+1, \n",
    "                tile_ranges=self.tile_ranges,\n",
    "                tile_ranges_done=self.tile_ranges_done\n",
    "            )\n",
    "        return self.children[action]\n",
    "    \n",
    "    def backup(self, value):\n",
    "        current = self\n",
    "        while current.parent is not None:\n",
    "            current.number_visits += 1\n",
    "            current.total_value += value\n",
    "            current = current.parent\n",
    "\n",
    "class RootParentNode:\n",
    "    def __init__(self, env):\n",
    "        self.parent = None\n",
    "        self.child_total_value = collections.defaultdict(float)\n",
    "        self.child_number_visits = collections.defaultdict(float)\n",
    "        self.env = env\n",
    "        self.reward = -np.inf\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, model, mcts_param):\n",
    "        self.model = model\n",
    "        self.temperature = mcts_param[\"temperature\"]\n",
    "        self.dir_epsilon = mcts_param[\"dirichlet_epsilon\"]\n",
    "        self.dir_noise = mcts_param[\"dirichlet_noise\"]\n",
    "        self.num_sims = mcts_param[\"num_simulations\"]\n",
    "        self.exploit = mcts_param[\"argmax_tree_policy\"]\n",
    "        self.add_dirichlet_noise = mcts_param[\"add_dirichlet_noise\"]\n",
    "        self.c_puct = mcts_param[\"puct_coefficient\"]\n",
    "    \n",
    "    def compute_action(self, node):\n",
    "        for _ in range(self.num_sims):\n",
    "            leaf = node.select()\n",
    "            if leaf.done:\n",
    "                value = leaf.reward\n",
    "            else:\n",
    "                child_priors, value = self.model(torch.tensor(leaf.state).unsqueeze(0))\n",
    "                child_priors = torch.softmax(child_priors, axis=1).squeeze(0).cpu().detach().numpy()\n",
    "                if self.add_dirichlet_noise:\n",
    "                    child_priors = (1 - self.dir_epsilon) * child_priors\n",
    "                    child_priors += self.dir_epsilon * np.random.dirichlet(\n",
    "                        [self.dir_noise] * child_priors.size\n",
    "                    )\n",
    "                \n",
    "                leaf.expand(child_priors)\n",
    "            leaf.backup(value)\n",
    "            \n",
    "        tree_policy = node.child_number_visits / node.number_visits\n",
    "        tree_policy = tree_policy / np.max(tree_policy)\n",
    "        tree_policy = np.power(tree_policy, self.temperature)\n",
    "        tree_policy = tree_policy / np.sum(tree_policy)\n",
    "        if self.exploit:\n",
    "            action = np.argmax(tree_policy)\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(node.action_space_size), p=tree_policy)\n",
    "        return tree_policy, action, node.children[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_v0(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(CNN_v0, self).__init__()\n",
    "        \n",
    "        self.convblock1 = nn.Sequential(\n",
    "            nn.Conv1d(5, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.convblock2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.convblock3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv1d(128, 50, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm1d(50), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(50 * 249, action_dim) # 4 * action_dim\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv1d(128, 50, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm1d(50), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(50 * 249, 128), \n",
    "            nn.Linear(128, 1), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        \n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = data_module.x_test[1].numpy()\n",
    "seqgame = SeqGame(sequence, standard_cnn)\n",
    "\n",
    "\n",
    "mcts_config = {\n",
    "    \"puct_coefficient\": 2.0,\n",
    "    \"num_simulations\": 10000,\n",
    "    \"temperature\": 1.5,\n",
    "    \"dirichlet_epsilon\": 0.25,\n",
    "    \"dirichlet_noise\": 0.03,\n",
    "    \"argmax_tree_policy\": False,\n",
    "    \"add_dirichlet_noise\": True,\n",
    "}\n",
    "\n",
    "model = CNN_v0(seqgame.action_size)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(model, mcts_config)\n",
    "state = seqgame.get_initial_state()\n",
    "tree_node = None\n",
    "while True:\n",
    "    print(state)\n",
    "    valid_moves = seqgame.get_valid_moves()\n",
    "    print(\"Valid moves\", [i for i in range(seqgame.action_size) if valid_moves[i]==1])\n",
    "    #action = int(input(\"Take action: \"))\n",
    "    #action = np.random.randint(0, 50)\n",
    "    if tree_node == None:\n",
    "        tree_node = Node(\n",
    "            state=seqgame.get_seq(), \n",
    "            reward=0, \n",
    "            done=False,\n",
    "            action=None,\n",
    "            parent=RootParentNode(env=seqgame),\n",
    "            mcts=mcts, \n",
    "            level=0, \n",
    "            tile_ranges=generate_tile_ranges(sequence.shape[1], 5, 5), \n",
    "            tile_ranges_done=[]\n",
    "        )\n",
    "    mcts_probs, action, tree_node = mcts.compute_action(tree_node)\n",
    "    print(mcts_probs)\n",
    "    #action = np.argmax(mcts_probs)\n",
    "    print(f\"This is the action: {action}\")\n",
    "    \n",
    "    if valid_moves[action] == 0:\n",
    "        print(\"Invalid action\")\n",
    "        continue\n",
    "    \n",
    "    state = seqgame.get_next_state(action)\n",
    "    print(f\"The current level is: {seqgame.current_level}\")\n",
    "    \n",
    "    print(seqgame.get_score())\n",
    "    #is_terminal = seqgame.terminate()\n",
    "    is_terminal = tree_node.done\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(\"Game ended\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current sequence: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [2.8930776e-06 2.8930776e-06 2.8930776e-06 5.3149238e-06 2.8930776e-06\n",
      " 2.8930776e-06 2.8930776e-06 5.3149238e-06 2.8930776e-06 2.8930776e-06\n",
      " 2.8930776e-06 2.8930776e-06 2.8930776e-06 5.3580541e-05 5.3149238e-06\n",
      " 2.8930776e-06 2.8930776e-06 2.8930776e-06 2.8930776e-06 2.8930776e-06\n",
      " 2.8930776e-06 2.8930776e-06 5.3149238e-06 2.8930776e-06 2.8930776e-06\n",
      " 2.8930776e-06 2.8930776e-06 2.8930776e-06 2.8930776e-06 9.9962509e-01\n",
      " 5.3149238e-06 2.8930776e-06 2.8930776e-06 2.8930776e-06 2.8930776e-06\n",
      " 6.5462875e-05 2.8930776e-06 2.8930776e-06 2.8930776e-06 1.0554779e-04\n",
      " 2.8930776e-06 2.8930776e-06 2.8930776e-06 2.8930776e-06 2.8930776e-06\n",
      " 2.8930776e-06 8.1828593e-06 2.8930776e-06 2.8930776e-06 2.8930776e-06]\n",
      "Selected action: 29\n",
      "0.6419474589153039\n",
      "1\n",
      "Current sequence: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [1.0397039e-02 6.2037790e-03 8.8199563e-03 4.6220697e-03 9.4132632e-04\n",
      " 2.3270478e-03 2.3077899e-03 7.0509277e-03 1.5156391e-03 7.6449987e-06\n",
      " 2.3657242e-03 1.1175628e-03 4.5254431e-03 3.6646845e-03 2.9706869e-03\n",
      " 8.2491845e-01 1.3486731e-02 1.7731802e-03 2.1181698e-03 2.9706869e-03\n",
      " 6.6912784e-03 1.2093897e-03 3.9832457e-03 5.3935698e-03 5.0153108e-03\n",
      " 7.6449987e-06 1.1945311e-04 7.5591537e-03 2.8873994e-03 0.0000000e+00\n",
      " 2.0809008e-03 2.7435604e-03 2.3657242e-03 2.9289443e-03 5.2918061e-03\n",
      " 2.5027520e-03 3.3987663e-03 3.9601992e-03 2.3657242e-03 7.6163472e-03\n",
      " 2.4830196e-03 7.6449987e-06 9.5562486e-04 7.6449987e-06 6.7187203e-03\n",
      " 4.6706372e-03 7.6449987e-06 4.1692154e-03 1.7031914e-03 7.0509277e-03]\n",
      "Selected action: 15\n",
      "0.6168028094051025\n",
      "2\n",
      "Game ended.\n"
     ]
    }
   ],
   "source": [
    "sequence = data_module.x_test[1].numpy()\n",
    "seqgame = SeqGame(sequence, standard_cnn)\n",
    "\n",
    "\n",
    "mcts_config = {\n",
    "    \"puct_coefficient\": 2.0,\n",
    "    \"num_simulations\": 10000,\n",
    "    \"temperature\": 1.5,\n",
    "    \"dirichlet_epsilon\": 0.25,\n",
    "    \"dirichlet_noise\": 0.03,\n",
    "    \"argmax_tree_policy\": False,\n",
    "    \"add_dirichlet_noise\": True,\n",
    "}\n",
    "\n",
    "model = CNN_v0(seqgame.action_size)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(model, mcts_config)\n",
    "\n",
    "# Get the initial sequence and create the root node\n",
    "initial_sequence = seqgame.get_seq()\n",
    "tile_ranges=generate_tile_ranges(sequence.shape[1], 5, 5)\n",
    "tile_ranges_done=[]\n",
    "prev_score = 0\n",
    "    \n",
    "root_node = Node(\n",
    "    state=initial_sequence,\n",
    "    reward=0,\n",
    "    done=False,\n",
    "    action=None,\n",
    "    parent=RootParentNode(env=seqgame),\n",
    "    mcts=mcts, \n",
    "    level=0, \n",
    "    tile_ranges=generate_tile_ranges(sequence.shape[1], 5, 5),\n",
    "    tile_ranges_done=[]\n",
    ")\n",
    "\n",
    "while not root_node.done:  # Loop until the root node indicates the game is done\n",
    "    print(\"Current sequence:\", convert_elements(root_node.state[-1,:]))\n",
    "    valid_moves = seqgame.get_valid_moves()\n",
    "    print(\"Valid moves:\", [i for i in range(seqgame.action_size) if valid_moves[i] == 1])\n",
    "\n",
    "    # Perform simulations and select an action using MCTS\n",
    "    mcts_probs, action, next_node = mcts.compute_action(root_node)\n",
    "    print(\"MCTS probabilities:\", mcts_probs)\n",
    "    print(\"Selected action:\", action)\n",
    "\n",
    "    if valid_moves[action] == 0:\n",
    "        print(\"Invalid action, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # next_state = seqgame.get_next_state(action)\n",
    "    next_state = seqgame.get_next_state(action, tile_ranges, tile_ranges_done)\n",
    "    tile_ranges_done.append(tile_ranges[action])\n",
    "    tile_ranges[action] = [0,0]\n",
    "    next_reward = seqgame.get_score()\n",
    "    print(next_reward)\n",
    "    print(next_node.level)\n",
    "    next_done = seqgame.terminate(next_node.level, next_reward, prev_score)  # Update termination status based on your logic\n",
    "    prev_score = next_reward\n",
    "\n",
    "    # Create the new child node and update the root node\n",
    "    next_child_node = root_node.get_child(action)\n",
    "    next_child_node.state = next_state\n",
    "    next_child_node.reward = next_reward\n",
    "    next_child_node.done = next_done\n",
    "    root_node = next_child_node\n",
    "\n",
    "print(\"Game ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
