{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from tqdm import tqdm\n",
    "import copy, math\n",
    "import collections\n",
    "\n",
    "from cremerl import utils, model_zoo, shuffle\n",
    "\n",
    "import shuffle_test\n",
    "\n",
    "#import gymnasium as gym\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set the logging level to WARNING\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = 'DeepSTARR'\n",
    "\n",
    "# load data\n",
    "data_path = '../../data/'\n",
    "filepath = os.path.join(data_path, expt_name+'_data.h5')\n",
    "data_module = utils.H5DataModule(filepath, batch_size=100, lower_case=False, transpose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyu/.conda/envs/tf_2/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "2023-08-13 08:52:53.103475: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-13 08:52:53.615592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/yiyu/.conda/envs/tf_2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc804ff60a74c0791a291891bface8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepstarr2 = model_zoo.deepstarr(2)\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer_dict = utils.configure_optimizer(deepstarr2, lr=0.001, weight_decay=1e-6, decay_factor=0.1, patience=5, monitor='val_loss')\n",
    "standard_cnn = model_zoo.DeepSTARR(deepstarr2,\n",
    "                                  criterion=loss,\n",
    "                                  optimizer=optimizer_dict)\n",
    "\n",
    "# load checkpoint for model with best validation performance\n",
    "standard_cnn = utils.load_model_from_checkpoint(standard_cnn, 'DeepSTARR_standard.ckpt')\n",
    "\n",
    "# evaluate best model\n",
    "pred = utils.get_predictions(standard_cnn, data_module.x_test[np.newaxis,100], batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swap_greedy(x, x_mut, tile_ranges):\n",
    "    ori = x.copy()\n",
    "    mut = x_mut.copy()\n",
    "    for tile_range in tile_ranges:\n",
    "        ori[:, tile_range[0]:tile_range[1]] = x_mut[:, tile_range[0]:tile_range[1]]\n",
    "        mut[:, tile_range[0]:tile_range[1]] = x[:, tile_range[0]:tile_range[1]]\n",
    "\n",
    "    return ori, mut\n",
    "\n",
    "def get_score(pred):\n",
    "    score1 = pred[0] - pred[2]\n",
    "    score2 = pred[3] - pred[1]\n",
    "    return (score1+score2)[0], score1+score2\n",
    "\n",
    "def generate_tile_ranges(sequence_length, window_size, stride):\n",
    "    ranges = []\n",
    "    start = np.arange(0, sequence_length - window_size + stride, stride)\n",
    "\n",
    "    for s in start:\n",
    "        e = min(s + window_size, sequence_length)\n",
    "        ranges.append([s, e])\n",
    "\n",
    "    if start[-1] + window_size - stride < sequence_length:  # Adjust the last range\n",
    "        ranges[-1][1] = sequence_length\n",
    "\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, tile_range, tile_ranges_ori, trials):\n",
    "    test_batch = []\n",
    "    for i in range(trials):\n",
    "        test_batch.append(x)\n",
    "        x_mut = shuffle.dinuc_shuffle(x.copy())\n",
    "        test_batch.append(x_mut)\n",
    "\n",
    "        ori = x.copy()\n",
    "        mut = x_mut.copy()\n",
    "        \n",
    "        ori, mut = get_swap_greedy(ori, mut, tile_ranges_ori)\n",
    "        \n",
    "        ori[:, tile_range[0]:tile_range[1]] = x_mut[:, tile_range[0]:tile_range[1]]\n",
    "        mut[:, tile_range[0]:tile_range[1]] = x[:, tile_range[0]:tile_range[1]]\n",
    "        \n",
    "        test_batch.append(ori)\n",
    "        test_batch.append(mut)\n",
    "\n",
    "    #print(np.array(test_batch).shape)\n",
    "    return np.array(test_batch)\n",
    "\n",
    "\n",
    "def get_batch_score(pred, trials):\n",
    "\n",
    "    score = []\n",
    "    score_sep = []\n",
    "    for i in range(0, pred.shape[0], 2):\n",
    "        # print(f\"Viewing number {i}\")\n",
    "        score1 = pred[0] - pred[i]\n",
    "        score2 = pred[i+1] - pred[1]\n",
    "        score.append((np.sum((score1, score2)[0])).tolist()) #np.sum(score1+score2, keepdims=True)\n",
    "        score_sep.append((score1+score2).tolist())\n",
    "        \n",
    "    # print(score)\n",
    "        \n",
    "    final = np.sum(np.array(score), axis=0)/trials\n",
    "\n",
    "    #max_ind = np.argmax(final)\n",
    "    #block_ind = np.argmax(np.array(score)[:, max_ind])\n",
    "    #print(np.array(total_score)[:, max_ind])\n",
    "    total_score_sep = np.sum(np.array(score_sep), axis=0)/trials\n",
    "\n",
    "    #print(np.max(score))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_sequence(one_hot_sequence):\n",
    "    A, L = one_hot_sequence.shape\n",
    "\n",
    "    # Create an all-ones row\n",
    "    ones_row = np.zeros(L)\n",
    "\n",
    "    # Add the all-ones row to the original sequence\n",
    "    new_sequence = np.vstack((one_hot_sequence, ones_row))\n",
    "\n",
    "    return np.array(new_sequence, dtype='float32')\n",
    "\n",
    "def taking_action(sequence_with_ones, tile_range):\n",
    "    start_idx, end_idx = tile_range\n",
    "\n",
    "    # Ensure the start_idx and end_idx are within valid bounds\n",
    "    #if start_idx < 0 or start_idx >= sequence_with_ones.shape[1] or end_idx < 0 or end_idx >= sequence_with_ones.shape[1]:\n",
    "    #    raise ValueError(\"Invalid tile range indices.\")\n",
    "\n",
    "    # Copy the input sequence to avoid modifying the original sequence\n",
    "    modified_sequence = sequence_with_ones.copy()\n",
    "\n",
    "    # Modify the last row within the specified tile range\n",
    "    modified_sequence[-1, start_idx:end_idx] = 1\n",
    "\n",
    "    return np.array(modified_sequence, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_elements(input_list):\n",
    "    input_list = input_list.tolist()\n",
    "    num_columns = 5  # Number of elements to process in each group\n",
    "\n",
    "    # Calculate the number of elements needed to pad the list\n",
    "    padding_length = num_columns - (len(input_list) % num_columns)\n",
    "    last_value = input_list[-1]\n",
    "    padded_list = input_list + [last_value] * padding_length\n",
    "\n",
    "    # Convert the padded list to a NumPy array for efficient operations\n",
    "    input_array = np.array(padded_list)\n",
    "    reshaped_array = input_array.reshape(-1, num_columns)\n",
    "\n",
    "    # Check if each row has the same value (all 0s or all 1s)\n",
    "    row_all_zeros = np.all(reshaped_array == 0, axis=1)\n",
    "    row_all_ones = np.all(reshaped_array == 1, axis=1)\n",
    "\n",
    "    # Replace all 0s with 0 and all 1s with 1 in the result array\n",
    "    output_array = np.where(row_all_zeros, 0, np.where(row_all_ones, 1, reshaped_array[:, 0]))\n",
    "\n",
    "    # Flatten the result array to get the final output list\n",
    "    output_list = output_array.flatten()\n",
    "\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqGame:\n",
    "    def __init__(self, sequence, model_func):\n",
    "        self.seq = sequence\n",
    "        self.ori_seq = sequence.copy()\n",
    "        self.tile_ranges = generate_tile_ranges(sequence.shape[1], 5, 5)\n",
    "        self.levels = 20\n",
    "        self.num_trials = 10\n",
    "        self.action_size = 50\n",
    "        \n",
    "        self.prev_score = -float(\"inf\")\n",
    "        self.current_score = 0\n",
    "        \n",
    "        self.trainer = pl.Trainer(accelerator='gpu', devices='1', logger=None, enable_progress_bar=False)\n",
    "        self.model = model_func\n",
    "        \n",
    "        if self.seq.shape[0]!=5:\n",
    "            self.seq = extend_sequence(self.seq)\n",
    "            self.ori_seq = extend_sequence(self.ori_seq)\n",
    "        \n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        self.seq = self.ori_seq.copy()\n",
    "        \n",
    "        return self.seq\n",
    "    \n",
    "    \n",
    "    def get_next_state(self, action, tile_ranges_done):\n",
    "        self.prev_score = self.current_score\n",
    "        # self.current_level += 1\n",
    "        \n",
    "        self.seq = taking_action(self.seq, self.tile_ranges[action])\n",
    "        \n",
    "        batch = get_batch(self.seq[:4, :], self.tile_ranges[action], tile_ranges_done, self.num_trials)\n",
    "        dataloader = torch.utils.data.DataLoader(batch, batch_size=100, shuffle=False)\n",
    "        pred = np.concatenate(self.trainer.predict(self.model, dataloaders=dataloader))\n",
    "        \n",
    "        self.current_score = np.tanh(1 * get_batch_score(pred, self.num_trials)) #ADDED TANH\n",
    "        \n",
    "        return self.seq\n",
    "    \n",
    "    def get_valid_moves(self):\n",
    "        return (convert_elements(self.seq[-1, :]) == 0).astype(np.uint8)\n",
    "    \n",
    "    def terminate(self, level, current_score, parent_score): #state\n",
    "        # if self.current_level >= self.levels:\n",
    "        #     return True\n",
    "        # if self.current_score < self.prev_score:\n",
    "        #     return True\n",
    "        \n",
    "        if level >= self.levels:\n",
    "            return True\n",
    "        if current_score < parent_score:\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "    \n",
    "    def set_seq(self, seq):\n",
    "        self.seq = seq\n",
    "    \n",
    "    def get_seq(self):\n",
    "        return self.seq.copy()\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, action, state, done, reward, mcts, level, tile_ranges_done, parent=None):\n",
    "        self.env = parent.env\n",
    "        self.action = action\n",
    "        \n",
    "        self.is_expanded = False\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        \n",
    "        self.action_space_size = self.env.action_size\n",
    "        self.child_total_value = np.zeros(\n",
    "            [self.action_space_size], dtype=np.float32\n",
    "        ) # Q\n",
    "        self.child_priors = np.zeros([self.action_space_size], dtype=np.float32) # P\n",
    "        self.child_number_visits = np.zeros(\n",
    "            [self.action_space_size], dtype=np.float32\n",
    "        ) # N\n",
    "        self.valid_actions = (convert_elements(state[-1, :]) == 0).astype(np.uint8)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "        self.state = state\n",
    "        self.level = level\n",
    "        \n",
    "        self.tile_ranges_done = tile_ranges_done\n",
    "        \n",
    "        self.mcts = mcts\n",
    "    \n",
    "    @property\n",
    "    def number_visits(self):\n",
    "        return self.parent.child_number_visits[self.action]\n",
    "    \n",
    "    @number_visits.setter\n",
    "    def number_visits(self, value):\n",
    "        self.parent.child_number_visits[self.action] = value\n",
    "        \n",
    "    @property\n",
    "    def total_value(self):\n",
    "        return self.parent.child_total_value[self.action]\n",
    "    \n",
    "    @total_value.setter\n",
    "    def total_value(self, value):\n",
    "        self.parent.child_total_value[self.action] = value\n",
    "        \n",
    "    def child_Q(self):\n",
    "        return self.child_total_value / (1 + self.child_number_visits)\n",
    "    \n",
    "    def child_U(self):\n",
    "        return (\n",
    "            math.sqrt(self.number_visits)\n",
    "            * self.child_priors\n",
    "            / (1 + self.child_number_visits)\n",
    "        )\n",
    "    \n",
    "    def best_action(self):\n",
    "        child_score = self.child_Q() + self.mcts.c_puct * self.child_U()\n",
    "        masked_child_score = child_score\n",
    "        # masked_child_score[~self.valid_actions] = -np.inf\n",
    "        masked_child_score = masked_child_score * self.valid_actions\n",
    "        return np.argmax(masked_child_score)\n",
    "    \n",
    "    def select(self):\n",
    "        current_node = self\n",
    "        while current_node.is_expanded:\n",
    "            best_action = current_node.best_action()\n",
    "            current_node = current_node.get_child(best_action)\n",
    "        return current_node\n",
    "    \n",
    "    def expand(self, child_priors):\n",
    "        self.is_expanded = True\n",
    "        self.child_priors = child_priors\n",
    "        \n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "        self.valid_actions = (convert_elements(state[-1, :]) == 0).astype(np.uint8)\n",
    "    \n",
    "    def get_child(self, action):\n",
    "        if action not in self.children:\n",
    "\n",
    "            self.env.set_seq(self.state.copy())\n",
    "            next_state = self.env.get_next_state(action, self.tile_ranges_done)\n",
    "            # self.tile_ranges_done.append(self.tile_ranges.pop(action))\n",
    "            new_tile_ranges_done = copy.deepcopy(self.tile_ranges_done)\n",
    "            # print(new_tile_ranges_done)\n",
    "            new_tile_ranges_done.append(self.env.tile_ranges[action])\n",
    "            # swap tile_ranges\n",
    "            reward = self.env.get_score()\n",
    "            terminated = self.env.terminate(self.level, reward, self.parent.reward)\n",
    "            self.children[action] = Node(\n",
    "                state=next_state, \n",
    "                action=action, \n",
    "                parent=self, \n",
    "                reward=reward,\n",
    "                done=terminated,\n",
    "                mcts=self.mcts, \n",
    "                level=self.level+1, \n",
    "                tile_ranges_done=new_tile_ranges_done\n",
    "            )\n",
    "        return self.children[action]\n",
    "    \n",
    "    def backup(self, value):\n",
    "        current = self\n",
    "        while current.parent is not None:\n",
    "            current.number_visits += 1\n",
    "            current.total_value += value\n",
    "            current = current.parent\n",
    "\n",
    "class RootParentNode:\n",
    "    def __init__(self, env):\n",
    "        self.parent = None\n",
    "        self.child_total_value = collections.defaultdict(float)\n",
    "        self.child_number_visits = collections.defaultdict(float)\n",
    "        self.env = env\n",
    "        self.reward = -np.inf\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, model, mcts_param):\n",
    "        self.model = model\n",
    "        self.temperature = mcts_param[\"temperature\"]\n",
    "        self.dir_epsilon = mcts_param[\"dirichlet_epsilon\"]\n",
    "        self.dir_noise = mcts_param[\"dirichlet_noise\"]\n",
    "        self.num_sims = mcts_param[\"num_simulations\"]\n",
    "        self.exploit = mcts_param[\"argmax_tree_policy\"]\n",
    "        self.add_dirichlet_noise = mcts_param[\"add_dirichlet_noise\"]\n",
    "        self.c_puct = mcts_param[\"puct_coefficient\"]\n",
    "    \n",
    "    def compute_action(self, node):\n",
    "        for _ in range(self.num_sims):\n",
    "            leaf = node.select()\n",
    "            if leaf.done:\n",
    "                value = leaf.reward\n",
    "            else:\n",
    "                child_priors, value = self.model(torch.tensor(leaf.state).unsqueeze(0))\n",
    "                child_priors = torch.softmax(child_priors, axis=1).squeeze(0).cpu().detach().numpy()\n",
    "                if self.add_dirichlet_noise:\n",
    "                    child_priors = (1 - self.dir_epsilon) * child_priors\n",
    "                    child_priors += self.dir_epsilon * np.random.dirichlet(\n",
    "                        [self.dir_noise] * child_priors.size\n",
    "                    )\n",
    "                \n",
    "                leaf.expand(child_priors)\n",
    "            leaf.backup(value)\n",
    "            \n",
    "        tree_policy = node.child_number_visits / node.number_visits\n",
    "        tree_policy = tree_policy / np.max(tree_policy)\n",
    "        tree_policy = np.power(tree_policy, self.temperature)\n",
    "        tree_policy = tree_policy / np.sum(tree_policy)\n",
    "        if self.exploit:\n",
    "            action = np.argmax(tree_policy)\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(node.action_space_size), p=tree_policy)\n",
    "        return tree_policy, action, node.children[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_v0(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(CNN_v0, self).__init__()\n",
    "        \n",
    "        self.convblock1 = nn.Sequential(\n",
    "            nn.Conv1d(5, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.convblock2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.convblock3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv1d(128, 50, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm1d(50), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(50 * 249, action_dim) # 4 * action_dim\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv1d(128, 50, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm1d(50), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(50 * 249, 128), \n",
    "            nn.Linear(128, 1), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        \n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current sequence: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [3.8486069e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06\n",
      " 7.0703422e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06\n",
      " 5.6562738e-05 3.8486069e-06 4.9641803e-05 1.0885504e-05 7.0703422e-06\n",
      " 7.0703422e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06\n",
      " 7.0703422e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06\n",
      " 3.8486069e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06\n",
      " 3.0788855e-05 7.0703422e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06\n",
      " 1.5212954e-05 7.0703422e-06 7.0703422e-06 8.0220288e-01 4.8107581e-04\n",
      " 1.9686900e-01 7.0703422e-06 7.0703422e-06 7.0703422e-06 7.0703422e-06\n",
      " 7.0703422e-06 7.0703422e-06 7.0703422e-06 3.8486069e-06 7.0703422e-06]\n",
      "Selected action: 38\n",
      "[]\n",
      "0\n",
      "Current sequence: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [1.0223289e-05 4.2905331e-01 5.5648538e-06 1.0223289e-05 1.2882816e-05\n",
      " 5.5648538e-06 5.5648538e-06 5.5648538e-06 5.5648538e-06 5.5648538e-06\n",
      " 7.7771192e-06 5.5648538e-06 5.5648538e-06 5.5648538e-06 5.5648538e-06\n",
      " 5.5648538e-06 5.5648538e-06 7.7771192e-06 5.5648538e-06 3.2604592e-05\n",
      " 5.5648538e-06 3.6144784e-06 5.5648538e-06 5.5648538e-06 5.5648538e-06\n",
      " 5.5648538e-06 5.5648538e-06 5.5648538e-06 7.7771192e-06 1.2882816e-05\n",
      " 5.4752672e-01 5.5648538e-06 1.0223289e-05 5.5648538e-06 5.5648538e-06\n",
      " 7.7771192e-06 7.7771192e-06 5.5648538e-06 0.0000000e+00 2.3027763e-02\n",
      " 5.5648538e-06 5.5648538e-06 5.5648538e-06 5.5648538e-06 3.6144784e-06\n",
      " 5.5648538e-06 5.5648538e-06 5.5648538e-06 7.6728284e-05 7.7771192e-06]\n",
      "Selected action: 1\n",
      "[[190, 195]]\n",
      "0.534704858664779\n",
      "Current sequence: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [5.46341607e-06 0.00000000e+00 3.90930200e-06 5.46341607e-06\n",
      " 7.18184629e-06 3.90930200e-06 3.90930200e-06 5.46341607e-06\n",
      " 3.90930200e-06 1.10571755e-05 9.05016077e-06 3.90930200e-06\n",
      " 3.90930200e-06 3.90930200e-06 3.90930200e-06 9.05016077e-06\n",
      " 9.84166503e-01 3.90930200e-06 5.46341607e-06 3.90930200e-06\n",
      " 3.90930200e-06 5.46341607e-06 5.46341607e-06 5.46341607e-06\n",
      " 2.55977193e-05 3.90930200e-06 3.90930200e-06 2.27109951e-04\n",
      " 5.46341607e-06 3.90930200e-06 3.90930200e-06 3.90930200e-06\n",
      " 5.46341607e-06 3.90930200e-06 3.90930200e-06 5.46341607e-06\n",
      " 7.24012862e-05 3.90930200e-06 0.00000000e+00 3.90930200e-06\n",
      " 1.52909011e-02 9.05016077e-06 5.46341607e-06 3.90930200e-06\n",
      " 7.18184629e-06 7.18184629e-06 3.90930200e-06 3.90930200e-06\n",
      " 3.90930200e-06 3.90930200e-06]\n",
      "Selected action: 16\n",
      "[[190, 195], [5, 10]]\n",
      "0.560239001174045\n",
      "Current sequence: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [4.9181581e-06 0.0000000e+00 1.1066546e-02 9.8735261e-01 7.1700006e-06\n",
      " 2.9690016e-06 3.5345447e-04 9.6882295e-06 8.3976047e-06 2.9690016e-06\n",
      " 2.9690016e-06 3.9028541e-06 2.9690016e-06 4.9181581e-06 2.9690016e-06\n",
      " 2.7402451e-05 0.0000000e+00 3.9028541e-06 8.3976047e-06 2.9690016e-06\n",
      " 4.9181581e-06 6.0088364e-06 2.9690016e-06 2.9690016e-06 7.6239929e-04\n",
      " 6.0088364e-06 2.9690016e-06 1.2964119e-04 4.9181581e-06 2.9690016e-06\n",
      " 2.9690016e-06 2.9690016e-06 4.9181581e-06 4.9181581e-06 8.3976047e-06\n",
      " 2.9690016e-06 8.5566200e-05 2.9690016e-06 0.0000000e+00 2.5555499e-05\n",
      " 2.9690016e-06 3.9028541e-06 7.1700006e-06 2.9690016e-06 4.9181581e-06\n",
      " 9.6882295e-06 1.8613533e-05 6.0088364e-06 3.9028541e-06 3.9028541e-06]\n",
      "Selected action: 3\n",
      "[[190, 195], [5, 10], [80, 85]]\n",
      "0.6156557663283048\n",
      "Current sequence: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [2.5053916e-04 0.0000000e+00 8.8229244e-06 0.0000000e+00 5.0823119e-06\n",
      " 2.4954998e-05 2.1045175e-06 5.0823119e-06 6.8673085e-06 5.9524746e-06\n",
      " 1.5274830e-04 2.1045175e-06 2.7664601e-06 5.9524746e-06 2.1045175e-06\n",
      " 5.9524746e-06 0.0000000e+00 6.4777783e-03 2.7259163e-04 2.7664601e-06\n",
      " 2.1045175e-06 3.4861382e-06 1.0935394e-05 6.8673091e-03 2.7664601e-06\n",
      " 2.7664601e-06 2.1045175e-06 9.7887391e-01 5.9524746e-06 2.1045175e-06\n",
      " 1.2046962e-05 2.1045175e-06 5.9524746e-06 2.3529225e-05 2.7664601e-06\n",
      " 3.8976214e-05 9.8602886e-06 2.1045175e-06 0.0000000e+00 6.8392348e-03\n",
      " 2.1045175e-06 2.7664601e-06 9.8602886e-06 7.8247313e-06 2.1045175e-06\n",
      " 9.8602886e-06 5.9524746e-06 8.8229244e-06 2.1045175e-06 4.2592442e-06]\n",
      "Selected action: 27\n",
      "[[190, 195], [5, 10], [80, 85], [15, 20]]\n",
      "0.5746537048322922\n",
      "Current sequence: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [6.12425841e-02 0.00000000e+00 3.01048218e-04 0.00000000e+00\n",
      " 2.00268605e-05 2.85236933e-06 2.85236933e-06 2.85236933e-06\n",
      " 2.28189547e-05 3.42379790e-04 1.06436615e-04 5.47244672e-05\n",
      " 1.01664882e-05 1.73589669e-05 1.04062623e-04 1.01706552e-04\n",
      " 0.00000000e+00 2.85236933e-06 4.84401680e-04 2.85236933e-06\n",
      " 1.60214884e-04 2.00268605e-05 3.60117614e-04 2.85236933e-06\n",
      " 2.85236933e-06 1.21049547e-04 2.84356705e-04 0.00000000e+00\n",
      " 2.20860777e-04 1.01706552e-04 8.80247331e-04 9.02069628e-01\n",
      " 5.86322369e-03 7.91631101e-05 9.24641936e-05 2.85236933e-06\n",
      " 1.74061744e-04 4.90985767e-05 0.00000000e+00 6.66000787e-03\n",
      " 6.96501788e-03 5.09512283e-05 2.05839591e-04 6.33165892e-03\n",
      " 1.08828404e-04 1.86770303e-05 4.54626061e-05 1.88285965e-04\n",
      " 6.07299246e-03 4.54626061e-05]\n",
      "Selected action: 31\n",
      "[[190, 195], [5, 10], [80, 85], [15, 20], [135, 140]]\n",
      "0.9589078078264818\n",
      "Current sequence: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [2.3383140e-03 0.0000000e+00 1.3160587e-03 0.0000000e+00 5.7688844e-03\n",
      " 1.2186740e-02 4.6024495e-03 6.0558612e-03 5.5725168e-04 1.2987043e-03\n",
      " 2.8616991e-03 1.3604781e-04 3.0689102e-04 1.3279913e-02 1.3562793e-02\n",
      " 1.3543873e-02 0.0000000e+00 8.8051958e-03 5.9022004e-04 1.3562793e-02\n",
      " 5.9022004e-04 3.5515300e-03 8.5118851e-03 4.7751130e-03 1.3562793e-02\n",
      " 6.2381400e-04 4.7751130e-03 0.0000000e+00 1.4217830e-03 7.6567805e-01\n",
      " 1.3828603e-02 0.0000000e+00 1.2796710e-04 3.7904366e-04 1.7438715e-04\n",
      " 1.0966270e-02 1.3073736e-02 1.4655685e-02 0.0000000e+00 9.5260562e-04\n",
      " 4.0224654e-04 3.7346815e-03 8.0816908e-04 4.3022087e-06 1.0234179e-02\n",
      " 3.3595820e-03 8.1557204e-04 1.5104609e-02 6.6883699e-03 4.2590441e-04]\n",
      "Selected action: 37\n",
      "[[190, 195], [5, 10], [80, 85], [15, 20], [135, 140], [155, 160]]\n",
      "0.9932331309146762\n",
      "Current sequence: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [6.0807811e-03 0.0000000e+00 1.7548851e-05 0.0000000e+00 1.9620212e-04\n",
      " 6.2950589e-02 9.1795079e-02 5.4750163e-03 5.0038449e-04 4.4774098e-04\n",
      " 7.0062757e-02 3.6188903e-01 3.0779289e-03 1.4077205e-02 2.7322745e-02\n",
      " 3.9701108e-02 0.0000000e+00 6.9930858e-04 1.7548851e-05 4.4774098e-04\n",
      " 6.7711500e-03 1.7548851e-05 3.9708518e-04 1.2664028e-03 4.9635648e-05\n",
      " 2.6000743e-03 1.1780359e-02 0.0000000e+00 2.2685155e-02 1.7548851e-05\n",
      " 1.7548851e-05 0.0000000e+00 2.9904220e-02 2.1736067e-02 3.2778107e-02\n",
      " 2.8655423e-02 5.0257381e-02 0.0000000e+00 0.0000000e+00 2.7970306e-04\n",
      " 3.7252766e-04 3.4280631e-03 3.9708518e-04 3.0746564e-02 3.9708518e-04\n",
      " 5.5838514e-02 1.7548851e-05 6.6437959e-03 7.0282659e-03 1.1584063e-03]\n",
      "Selected action: 11\n",
      "[[190, 195], [5, 10], [80, 85], [15, 20], [135, 140], [155, 160], [185, 190]]\n",
      "0.9910977083592672\n",
      "Game ended.\n"
     ]
    }
   ],
   "source": [
    "sequence = data_module.x_test[1].numpy()\n",
    "seqgame = SeqGame(sequence, standard_cnn)\n",
    "\n",
    "\n",
    "mcts_config = {\n",
    "    \"puct_coefficient\": 2.0,\n",
    "    \"num_simulations\": 10000,\n",
    "    \"temperature\": 1.5,\n",
    "    \"dirichlet_epsilon\": 0.25,\n",
    "    \"dirichlet_noise\": 0.03,\n",
    "    \"argmax_tree_policy\": False,\n",
    "    \"add_dirichlet_noise\": True,\n",
    "}\n",
    "\n",
    "model = CNN_v0(seqgame.action_size)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(model, mcts_config)\n",
    "\n",
    "# Get the initial sequence and create the root node\n",
    "initial_sequence = seqgame.get_seq()\n",
    "    \n",
    "root_node = Node(\n",
    "    state=initial_sequence,\n",
    "    reward=0,\n",
    "    done=False,\n",
    "    action=None,\n",
    "    parent=RootParentNode(env=seqgame),\n",
    "    mcts=mcts, \n",
    "    level=0, \n",
    "    tile_ranges_done=[]\n",
    ")\n",
    "\n",
    "while not root_node.done:  # Loop until the root node indicates the game is done\n",
    "    print(\"Current sequence:\", convert_elements(root_node.state[-1,:]))\n",
    "    valid_moves = root_node.valid_actions\n",
    "    print(\"Valid moves:\", [i for i in range(seqgame.action_size) if valid_moves[i] == 1])\n",
    "\n",
    "    # Perform simulations and select an action using MCTS\n",
    "    mcts_probs, action, next_node = mcts.compute_action(root_node)\n",
    "    print(\"MCTS probabilities:\", mcts_probs)\n",
    "    print(\"Selected action:\", action)\n",
    "    \n",
    "\n",
    "    if valid_moves[action] == 0:\n",
    "        print(\"Invalid action, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(root_node.tile_ranges_done)\n",
    "    print(root_node.reward)\n",
    "\n",
    "    root_node = next_node\n",
    "\n",
    "print(\"Game ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaDNA:\n",
    "    def __init__(self, model, optimizer, env, args, mcts_config = {\n",
    "        \"puct_coefficient\": 2.0,\n",
    "        \"num_simulations\": 10000,\n",
    "        \"temperature\": 1.5,\n",
    "        \"dirichlet_epsilon\": 0.25,\n",
    "        \"dirichlet_noise\": 0.03,\n",
    "        \"argmax_tree_policy\": False,\n",
    "        \"add_dirichlet_noise\": True,}\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(model, mcts_config)\n",
    "        \n",
    "        self.initial_sequence = env.get_seq()\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "\n",
    "        root_node = Node(\n",
    "            state=self.initial_sequence,\n",
    "            reward=0,\n",
    "            done=False,\n",
    "            action=None,\n",
    "            parent=RootParentNode(env=seqgame),\n",
    "            mcts=self.mcts, \n",
    "            level=0, \n",
    "            tile_ranges_done=[]\n",
    "        )\n",
    "\n",
    "        while True:  # Loop until the root node indicates the game is done\n",
    "            valid_moves = root_node.valid_actions\n",
    "            mcts_probs, action, next_node = self.mcts.compute_action(root_node)\n",
    "            \n",
    "            memory.append((root_node.state, mcts_probs, next_node.reward))\n",
    "\n",
    "            if valid_moves[action] == 0:\n",
    "                print(\"Invalid action, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            if next_node.done:\n",
    "                return memory\n",
    "\n",
    "            root_node = next_node\n",
    "    \n",
    "    def train(self, memory):\n",
    "        np.random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            priors = nn.Softmax(dim=-1)(out_policy)\n",
    "            \n",
    "            policy_loss = torch.mean(\n",
    "                -torch.sum(policy_targets * torch.log(priors), dim=-1)\n",
    "            )\n",
    "            value_loss = torch.mean(torch.pow(value_targets - out_value, 2))\n",
    "            \n",
    "            total_loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            return total_loss\n",
    "        \n",
    "    def learn(self):\n",
    "        lowest_loss = np.inf\n",
    "        for iteration in tqdm(range(self.args['num_iterations'])):\n",
    "            print(f\"Iteration {iteration}:\")\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in tqdm(range(self.args['num_selfPlay_iterations'])):\n",
    "                print(f\"SelfPlay Iteration {selfPlay_iteration}\")\n",
    "                memory += self.selfPlay()\n",
    "            \n",
    "            self.model.train()\n",
    "            for epoch in tqdm(range(self.args['num_epochs'])):\n",
    "                print(f\"Training Epoch {epoch}\")\n",
    "                loss = self.train(memory)\n",
    "                print(f\"Total Loss: {loss}\")\n",
    "                \n",
    "                if loss < lowest_loss:\n",
    "                    print(\"Saving the best model\")\n",
    "                    lowest_loss = loss\n",
    "                    torch.save(self.model.state_dict(), \"best_model.pt\")\n",
    "                    torch.save(self.optimizer.state_dict(), \"best_optimizer.pt\")\n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.32s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 82.49it/s]\n",
      "  5%|▌         | 1/20 [00:06<02:07,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 4.044436454772949\n",
      "Saving the best model\n",
      "Training Epoch 1\n",
      "Total Loss: 24.300443649291992\n",
      "Training Epoch 2\n",
      "Total Loss: 4.5381646156311035\n",
      "Training Epoch 3\n",
      "Total Loss: 31.199615478515625\n",
      "Iteration 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:33<00:00, 16.56s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 125.83it/s]\n",
      " 10%|█         | 2/20 [00:39<06:40, 22.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 23.918163299560547\n",
      "Training Epoch 1\n",
      "Total Loss: 18.642078399658203\n",
      "Training Epoch 2\n",
      "Total Loss: 25.733264923095703\n",
      "Training Epoch 3\n",
      "Total Loss: 14.73617935180664\n",
      "Iteration 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:22<00:00, 11.40s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 121.94it/s]\n",
      " 15%|█▌        | 3/20 [01:02<06:22, 22.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 18.64011573791504\n",
      "Training Epoch 1\n",
      "Total Loss: 7.696557521820068\n",
      "Training Epoch 2\n",
      "Total Loss: 14.106721878051758\n",
      "Training Epoch 3\n",
      "Total Loss: 9.862923622131348\n",
      "Iteration 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:24<00:00, 12.19s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 130.49it/s]\n",
      " 20%|██        | 4/20 [01:27<06:12, 23.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 9.24693775177002\n",
      "Training Epoch 1\n",
      "Total Loss: 6.565601348876953\n",
      "Training Epoch 2\n",
      "Total Loss: 4.7549214363098145\n",
      "Training Epoch 3\n",
      "Total Loss: 15.290238380432129\n",
      "Iteration 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:26<00:00, 13.14s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 103.92it/s]\n",
      " 25%|██▌       | 5/20 [01:53<06:05, 24.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 9.489765167236328\n",
      "Training Epoch 1\n",
      "Total Loss: 4.478972434997559\n",
      "Training Epoch 2\n",
      "Total Loss: 6.852736473083496\n",
      "Training Epoch 3\n",
      "Total Loss: 8.284147262573242\n",
      "Iteration 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:20<00:00, 10.39s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 81.20it/s]\n",
      " 30%|███       | 6/20 [02:14<05:24, 23.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 4.759649276733398\n",
      "Training Epoch 1\n",
      "Total Loss: 2.9644315242767334\n",
      "Saving the best model\n",
      "Training Epoch 2\n",
      "Total Loss: 6.511432647705078\n",
      "Training Epoch 3\n",
      "Total Loss: 3.1394615173339844\n",
      "Iteration 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.01s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 135.44it/s]\n",
      " 35%|███▌      | 7/20 [02:42<05:21, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 4.615832805633545\n",
      "Training Epoch 1\n",
      "Total Loss: 3.4260756969451904\n",
      "Training Epoch 2\n",
      "Total Loss: 7.478007793426514\n",
      "Training Epoch 3\n",
      "Total Loss: 4.24071741104126\n",
      "Iteration 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.00s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 133.56it/s]\n",
      " 40%|████      | 8/20 [02:58<04:23, 21.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 3.7998058795928955\n",
      "Training Epoch 1\n",
      "Total Loss: 9.363099098205566\n",
      "Training Epoch 2\n",
      "Total Loss: 8.146714210510254\n",
      "Training Epoch 3\n",
      "Total Loss: 9.270411491394043\n",
      "Iteration 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.59s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 85.42it/s]\n",
      " 45%|████▌     | 9/20 [03:17<03:52, 21.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 3.6013286113739014\n",
      "Training Epoch 1\n",
      "Total Loss: 8.78450870513916\n",
      "Training Epoch 2\n",
      "Total Loss: 2.9380550384521484\n",
      "Saving the best model\n",
      "Training Epoch 3\n",
      "Total Loss: 6.301684379577637\n",
      "Iteration 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:09<00:00,  4.91s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 122.51it/s]\n",
      " 50%|█████     | 10/20 [03:27<02:56, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 15.75601863861084\n",
      "Training Epoch 1\n",
      "Total Loss: 6.376691818237305\n",
      "Training Epoch 2\n",
      "Total Loss: 4.002157688140869\n",
      "Training Epoch 3\n",
      "Total Loss: 3.484745740890503\n",
      "Iteration 10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.23s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 57.45it/s]\n",
      " 55%|█████▌    | 11/20 [03:41<02:30, 16.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 2.0403265953063965\n",
      "Saving the best model\n",
      "Training Epoch 1\n",
      "Total Loss: 3.0007247924804688\n",
      "Training Epoch 2\n",
      "Total Loss: 3.083150863647461\n",
      "Training Epoch 3\n",
      "Total Loss: 1.6913474798202515\n",
      "Saving the best model\n",
      "Iteration 11:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:15<00:00,  7.64s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 87.14it/s]\n",
      " 60%|██████    | 12/20 [03:57<02:10, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 5.642356872558594\n",
      "Training Epoch 1\n",
      "Total Loss: 3.1234312057495117\n",
      "Training Epoch 2\n",
      "Total Loss: 3.077346086502075\n",
      "Training Epoch 3\n",
      "Total Loss: 0.6629111766815186\n",
      "Saving the best model\n",
      "Iteration 12:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.39s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 135.37it/s]\n",
      " 65%|██████▌   | 13/20 [04:10<01:46, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 0.8353189826011658\n",
      "Training Epoch 1\n",
      "Total Loss: 1.6130762100219727\n",
      "Training Epoch 2\n",
      "Total Loss: 1.1154594421386719\n",
      "Training Epoch 3\n",
      "Total Loss: 3.1989974975585938\n",
      "Iteration 13:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.94s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 143.78it/s]\n",
      " 70%|███████   | 14/20 [04:18<01:18, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 1.60453200340271\n",
      "Training Epoch 1\n",
      "Total Loss: 0.8593326210975647\n",
      "Training Epoch 2\n",
      "Total Loss: 0.695192813873291\n",
      "Training Epoch 3\n",
      "Total Loss: 0.8667343854904175\n",
      "Iteration 14:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.23s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 137.25it/s]\n",
      " 75%|███████▌  | 15/20 [04:26<00:58, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 2.7617459297180176\n",
      "Training Epoch 1\n",
      "Total Loss: 2.6297030448913574\n",
      "Training Epoch 2\n",
      "Total Loss: 1.9297664165496826\n",
      "Training Epoch 3\n",
      "Total Loss: 3.1099462509155273\n",
      "Iteration 15:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.91s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 135.41it/s]\n",
      " 80%|████████  | 16/20 [04:34<00:42, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 1.2656439542770386\n",
      "Training Epoch 1\n",
      "Total Loss: 11.56114673614502\n",
      "Training Epoch 2\n",
      "Total Loss: 12.43708610534668\n",
      "Training Epoch 3\n",
      "Total Loss: 11.874720573425293\n",
      "Iteration 16:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:09<00:00,  4.85s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 132.51it/s]\n",
      " 85%|████████▌ | 17/20 [04:44<00:30, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 0.9566527605056763\n",
      "Training Epoch 1\n",
      "Total Loss: 6.114522457122803\n",
      "Training Epoch 2\n",
      "Total Loss: 4.606730937957764\n",
      "Training Epoch 3\n",
      "Total Loss: 11.227452278137207\n",
      "Iteration 17:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.17s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 137.02it/s]\n",
      " 90%|█████████ | 18/20 [04:52<00:19,  9.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 1.998283863067627\n",
      "Training Epoch 1\n",
      "Total Loss: 1.9020521640777588\n",
      "Training Epoch 2\n",
      "Total Loss: 4.731648921966553\n",
      "Training Epoch 3\n",
      "Total Loss: 2.3454155921936035\n",
      "Iteration 18:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.00s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 118.84it/s]\n",
      " 95%|█████████▌| 19/20 [05:06<00:11, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 7.525950908660889\n",
      "Training Epoch 1\n",
      "Total Loss: 4.540889263153076\n",
      "Training Epoch 2\n",
      "Total Loss: 17.743812561035156\n",
      "Training Epoch 3\n",
      "Total Loss: 10.376371383666992\n",
      "Iteration 19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:09<00:00,  4.56s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 126.88it/s]\n",
      "100%|██████████| 20/20 [05:15<00:00, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 3.825197219848633\n",
      "Training Epoch 1\n",
      "Total Loss: 1.8178150653839111\n",
      "Training Epoch 2\n",
      "Total Loss: 11.940710067749023\n",
      "Training Epoch 3\n",
      "Total Loss: 10.696284294128418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sequence = data_module.x_test[1].numpy()\n",
    "seqgame = SeqGame(sequence, standard_cnn)\n",
    "\n",
    "model = CNN_v0(seqgame.action_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'num_iterations': 20, # 3 \n",
    "    'num_selfPlay_iterations': 2, # 500\n",
    "    'num_epochs': 4, \n",
    "    'batch_size': 2, # 64\n",
    "}\n",
    "\n",
    "mcts_config = {\n",
    "    \"puct_coefficient\": 2.0,\n",
    "    \"num_simulations\": 1000,\n",
    "    \"temperature\": 1.5,\n",
    "    \"dirichlet_epsilon\": 0.25,\n",
    "    \"dirichlet_noise\": 0.03,\n",
    "    \"argmax_tree_policy\": False,\n",
    "    \"add_dirichlet_noise\": True,\n",
    "}\n",
    "\n",
    "alphadna = AlphaDNA(model, optimizer, seqgame, args, mcts_config)\n",
    "alphadna.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
