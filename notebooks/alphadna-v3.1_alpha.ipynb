{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from tqdm import tqdm\n",
    "import copy, math\n",
    "import collections\n",
    "\n",
    "from cremerl import utils, model_zoo, shuffle\n",
    "\n",
    "import shuffle_test\n",
    "\n",
    "#import gymnasium as gym\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set the logging level to WARNING\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = 'DeepSTARR'\n",
    "\n",
    "# load data\n",
    "data_path = '../../data/'\n",
    "filepath = os.path.join(data_path, expt_name+'_data.h5')\n",
    "data_module = utils.H5DataModule(filepath, batch_size=100, lower_case=False, transpose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyu/.conda/envs/tf_2/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "2023-08-11 17:30:58.878604: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-11 17:30:59.386611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/yiyu/.conda/envs/tf_2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6368a8f6a5442299d76670fc262467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepstarr2 = model_zoo.deepstarr(2)\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer_dict = utils.configure_optimizer(deepstarr2, lr=0.001, weight_decay=1e-6, decay_factor=0.1, patience=5, monitor='val_loss')\n",
    "standard_cnn = model_zoo.DeepSTARR(deepstarr2,\n",
    "                                  criterion=loss,\n",
    "                                  optimizer=optimizer_dict)\n",
    "\n",
    "# load checkpoint for model with best validation performance\n",
    "standard_cnn = utils.load_model_from_checkpoint(standard_cnn, 'DeepSTARR_standard.ckpt')\n",
    "\n",
    "# evaluate best model\n",
    "pred = utils.get_predictions(standard_cnn, data_module.x_test[np.newaxis,100], batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swap_greedy(x, x_mut, tile_ranges):\n",
    "    ori = x.copy()\n",
    "    mut = x_mut.copy()\n",
    "    for tile_range in tile_ranges:\n",
    "        ori[:, tile_range[0]:tile_range[1]] = x_mut[:, tile_range[0]:tile_range[1]]\n",
    "        mut[:, tile_range[0]:tile_range[1]] = x[:, tile_range[0]:tile_range[1]]\n",
    "\n",
    "    return ori, mut\n",
    "\n",
    "def get_score(pred):\n",
    "    score1 = pred[0] - pred[2]\n",
    "    score2 = pred[3] - pred[1]\n",
    "    return (score1+score2)[0], score1+score2\n",
    "\n",
    "def generate_tile_ranges(sequence_length, window_size, stride):\n",
    "    ranges = []\n",
    "    start = np.arange(0, sequence_length - window_size + stride, stride)\n",
    "\n",
    "    for s in start:\n",
    "        e = min(s + window_size, sequence_length)\n",
    "        ranges.append([s, e])\n",
    "\n",
    "    if start[-1] + window_size - stride < sequence_length:  # Adjust the last range\n",
    "        ranges[-1][1] = sequence_length\n",
    "\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, tile_range, tile_ranges_ori, trials):\n",
    "    test_batch = []\n",
    "    for i in range(trials):\n",
    "        test_batch.append(x)\n",
    "        x_mut = shuffle.dinuc_shuffle(x.copy())\n",
    "        test_batch.append(x_mut)\n",
    "\n",
    "        ori = x.copy()\n",
    "        mut = x_mut.copy()\n",
    "        \n",
    "        ori, mut = get_swap_greedy(ori, mut, tile_ranges_ori)\n",
    "        \n",
    "        ori[:, tile_range[0]:tile_range[1]] = x_mut[:, tile_range[0]:tile_range[1]]\n",
    "        mut[:, tile_range[0]:tile_range[1]] = x[:, tile_range[0]:tile_range[1]]\n",
    "        \n",
    "        test_batch.append(ori)\n",
    "        test_batch.append(mut)\n",
    "\n",
    "    #print(np.array(test_batch).shape)\n",
    "    return np.array(test_batch)\n",
    "\n",
    "\n",
    "def get_batch_score(pred, trials):\n",
    "\n",
    "    score = []\n",
    "    score_sep = []\n",
    "    for i in range(0, pred.shape[0], 2):\n",
    "        # print(f\"Viewing number {i}\")\n",
    "        score1 = pred[0] - pred[i]\n",
    "        score2 = pred[i+1] - pred[1]\n",
    "        score.append((np.sum((score1, score2)[0])).tolist()) #np.sum(score1+score2, keepdims=True)\n",
    "        score_sep.append((score1+score2).tolist())\n",
    "        \n",
    "    # print(score)\n",
    "        \n",
    "    final = np.sum(np.array(score), axis=0)/trials\n",
    "\n",
    "    #max_ind = np.argmax(final)\n",
    "    #block_ind = np.argmax(np.array(score)[:, max_ind])\n",
    "    #print(np.array(total_score)[:, max_ind])\n",
    "    total_score_sep = np.sum(np.array(score_sep), axis=0)/trials\n",
    "\n",
    "    #print(np.max(score))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_sequence(one_hot_sequence):\n",
    "    A, L = one_hot_sequence.shape\n",
    "\n",
    "    # Create an all-ones row\n",
    "    ones_row = np.zeros(L)\n",
    "\n",
    "    # Add the all-ones row to the original sequence\n",
    "    new_sequence = np.vstack((one_hot_sequence, ones_row))\n",
    "\n",
    "    return np.array(new_sequence, dtype='float32')\n",
    "\n",
    "def taking_action(sequence_with_ones, tile_range):\n",
    "    start_idx, end_idx = tile_range\n",
    "\n",
    "    # Ensure the start_idx and end_idx are within valid bounds\n",
    "    #if start_idx < 0 or start_idx >= sequence_with_ones.shape[1] or end_idx < 0 or end_idx >= sequence_with_ones.shape[1]:\n",
    "    #    raise ValueError(\"Invalid tile range indices.\")\n",
    "\n",
    "    # Copy the input sequence to avoid modifying the original sequence\n",
    "    modified_sequence = sequence_with_ones.copy()\n",
    "\n",
    "    # Modify the last row within the specified tile range\n",
    "    modified_sequence[-1, start_idx:end_idx] = 1\n",
    "\n",
    "    return np.array(modified_sequence, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_elements(input_list):\n",
    "    input_list = input_list.tolist()\n",
    "    num_columns = 5  # Number of elements to process in each group\n",
    "\n",
    "    # Calculate the number of elements needed to pad the list\n",
    "    padding_length = num_columns - (len(input_list) % num_columns)\n",
    "    last_value = input_list[-1]\n",
    "    padded_list = input_list + [last_value] * padding_length\n",
    "\n",
    "    # Convert the padded list to a NumPy array for efficient operations\n",
    "    input_array = np.array(padded_list)\n",
    "    reshaped_array = input_array.reshape(-1, num_columns)\n",
    "\n",
    "    # Check if each row has the same value (all 0s or all 1s)\n",
    "    row_all_zeros = np.all(reshaped_array == 0, axis=1)\n",
    "    row_all_ones = np.all(reshaped_array == 1, axis=1)\n",
    "\n",
    "    # Replace all 0s with 0 and all 1s with 1 in the result array\n",
    "    output_array = np.where(row_all_zeros, 0, np.where(row_all_ones, 1, reshaped_array[:, 0]))\n",
    "\n",
    "    # Flatten the result array to get the final output list\n",
    "    output_list = output_array.flatten()\n",
    "\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqGame:\n",
    "    def __init__(self, sequence, model_func):\n",
    "        self.seq = sequence\n",
    "        self.ori_seq = sequence.copy()\n",
    "        self.tile_ranges = generate_tile_ranges(sequence.shape[1], 5, 5)\n",
    "        self.levels = 20\n",
    "        self.num_trials = 10\n",
    "        self.action_size = 50\n",
    "        \n",
    "        self.prev_score = -float(\"inf\")\n",
    "        self.current_score = 0\n",
    "        \n",
    "        self.trainer = pl.Trainer(accelerator='gpu', devices='1', logger=None, enable_progress_bar=False)\n",
    "        self.model = model_func\n",
    "        \n",
    "        if self.seq.shape[0]!=5:\n",
    "            self.seq = extend_sequence(self.seq)\n",
    "            self.ori_seq = extend_sequence(self.ori_seq)\n",
    "        \n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        self.seq = self.ori_seq.copy()\n",
    "        \n",
    "        return self.seq\n",
    "    \n",
    "    \n",
    "    def get_next_state(self, action, tile_ranges_done):\n",
    "        self.prev_score = self.current_score\n",
    "        # self.current_level += 1\n",
    "        \n",
    "        self.seq = taking_action(self.seq, self.tile_ranges[action])\n",
    "        \n",
    "        batch = get_batch(self.seq[:4, :], self.tile_ranges[action], tile_ranges_done, self.num_trials)\n",
    "        dataloader = torch.utils.data.DataLoader(batch, batch_size=100, shuffle=False)\n",
    "        pred = np.concatenate(self.trainer.predict(self.model, dataloaders=dataloader))\n",
    "        \n",
    "        self.current_score = np.tanh(1 * get_batch_score(pred, self.num_trials)) #ADDED TANH\n",
    "        \n",
    "        return self.seq\n",
    "    \n",
    "    def get_valid_moves(self):\n",
    "        return (convert_elements(self.seq[-1, :]) == 0).astype(np.uint8)\n",
    "    \n",
    "    def terminate(self, level, current_score, parent_score): #state\n",
    "        # if self.current_level >= self.levels:\n",
    "        #     return True\n",
    "        # if self.current_score < self.prev_score:\n",
    "        #     return True\n",
    "        \n",
    "        if level >= self.levels:\n",
    "            return True\n",
    "        if current_score < parent_score:\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "    \n",
    "    def set_seq(self, seq):\n",
    "        self.seq = seq\n",
    "    \n",
    "    def get_seq(self):\n",
    "        return self.seq.copy()\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, action, state, done, reward, mcts, level, tile_ranges_done, parent=None):\n",
    "        self.env = parent.env\n",
    "        self.action = action\n",
    "        \n",
    "        self.is_expanded = False\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        \n",
    "        self.action_space_size = self.env.action_size\n",
    "        self.child_total_value = np.zeros(\n",
    "            [self.action_space_size], dtype=np.float32\n",
    "        ) # Q\n",
    "        self.child_priors = np.zeros([self.action_space_size], dtype=np.float32) # P\n",
    "        self.child_number_visits = np.zeros(\n",
    "            [self.action_space_size], dtype=np.float32\n",
    "        ) # N\n",
    "        self.valid_actions = (convert_elements(state[-1, :]) == 0).astype(np.uint8)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "        self.state = state\n",
    "        self.level = level\n",
    "        \n",
    "        self.tile_ranges_done = tile_ranges_done\n",
    "        \n",
    "        self.mcts = mcts\n",
    "    \n",
    "    @property\n",
    "    def number_visits(self):\n",
    "        return self.parent.child_number_visits[self.action]\n",
    "    \n",
    "    @number_visits.setter\n",
    "    def number_visits(self, value):\n",
    "        self.parent.child_number_visits[self.action] = value\n",
    "        \n",
    "    @property\n",
    "    def total_value(self):\n",
    "        return self.parent.child_total_value[self.action]\n",
    "    \n",
    "    @total_value.setter\n",
    "    def total_value(self, value):\n",
    "        self.parent.child_total_value[self.action] = value\n",
    "        \n",
    "    def child_Q(self):\n",
    "        return self.child_total_value / (1 + self.child_number_visits)\n",
    "    \n",
    "    def child_U(self):\n",
    "        return (\n",
    "            math.sqrt(self.number_visits)\n",
    "            * self.child_priors\n",
    "            / (1 + self.child_number_visits)\n",
    "        )\n",
    "    \n",
    "    def best_action(self):\n",
    "        child_score = self.child_Q() + self.mcts.c_puct * self.child_U()\n",
    "        masked_child_score = child_score\n",
    "        # masked_child_score[~self.valid_actions] = -np.inf\n",
    "        masked_child_score = masked_child_score * self.valid_actions\n",
    "        return np.argmax(masked_child_score)\n",
    "    \n",
    "    def select(self):\n",
    "        current_node = self\n",
    "        while current_node.is_expanded:\n",
    "            best_action = current_node.best_action()\n",
    "            current_node = current_node.get_child(best_action)\n",
    "        return current_node\n",
    "    \n",
    "    def expand(self, child_priors):\n",
    "        self.is_expanded = True\n",
    "        self.child_priors = child_priors\n",
    "        \n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "        self.valid_actions = (convert_elements(state[-1, :]) == 0).astype(np.uint8)\n",
    "    \n",
    "    def get_child(self, action):\n",
    "        if action not in self.children:\n",
    "\n",
    "            self.env.set_seq(self.state.copy())\n",
    "            next_state = self.env.get_next_state(action, self.tile_ranges_done)\n",
    "            # self.tile_ranges_done.append(self.tile_ranges.pop(action))\n",
    "            new_tile_ranges_done = copy.deepcopy(self.tile_ranges_done)\n",
    "            # print(new_tile_ranges_done)\n",
    "            new_tile_ranges_done.append(self.env.tile_ranges[action])\n",
    "            # swap tile_ranges\n",
    "            reward = self.env.get_score()\n",
    "            terminated = self.env.terminate(self.level, reward, self.parent.reward)\n",
    "            self.children[action] = Node(\n",
    "                state=next_state, \n",
    "                action=action, \n",
    "                parent=self, \n",
    "                reward=reward,\n",
    "                done=terminated,\n",
    "                mcts=self.mcts, \n",
    "                level=self.level+1, \n",
    "                tile_ranges_done=new_tile_ranges_done\n",
    "            )\n",
    "        return self.children[action]\n",
    "    \n",
    "    def backup(self, value):\n",
    "        current = self\n",
    "        while current.parent is not None:\n",
    "            current.number_visits += 1\n",
    "            current.total_value += value\n",
    "            current = current.parent\n",
    "\n",
    "class RootParentNode:\n",
    "    def __init__(self, env):\n",
    "        self.parent = None\n",
    "        self.child_total_value = collections.defaultdict(float)\n",
    "        self.child_number_visits = collections.defaultdict(float)\n",
    "        self.env = env\n",
    "        self.reward = -np.inf\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, model, mcts_param):\n",
    "        self.model = model\n",
    "        self.temperature = mcts_param[\"temperature\"]\n",
    "        self.dir_epsilon = mcts_param[\"dirichlet_epsilon\"]\n",
    "        self.dir_noise = mcts_param[\"dirichlet_noise\"]\n",
    "        self.num_sims = mcts_param[\"num_simulations\"]\n",
    "        self.exploit = mcts_param[\"argmax_tree_policy\"]\n",
    "        self.add_dirichlet_noise = mcts_param[\"add_dirichlet_noise\"]\n",
    "        self.c_puct = mcts_param[\"puct_coefficient\"]\n",
    "    \n",
    "    def compute_action(self, node):\n",
    "        for _ in range(self.num_sims):\n",
    "            leaf = node.select()\n",
    "            if leaf.done:\n",
    "                value = leaf.reward\n",
    "            else:\n",
    "                child_priors, value = self.model(torch.tensor(leaf.state).unsqueeze(0))\n",
    "                child_priors = torch.softmax(child_priors, axis=1).squeeze(0).cpu().detach().numpy()\n",
    "                if self.add_dirichlet_noise:\n",
    "                    child_priors = (1 - self.dir_epsilon) * child_priors\n",
    "                    child_priors += self.dir_epsilon * np.random.dirichlet(\n",
    "                        [self.dir_noise] * child_priors.size\n",
    "                    )\n",
    "                \n",
    "                leaf.expand(child_priors)\n",
    "            leaf.backup(value)\n",
    "            \n",
    "        tree_policy = node.child_number_visits / node.number_visits\n",
    "        tree_policy = tree_policy / np.max(tree_policy)\n",
    "        tree_policy = np.power(tree_policy, self.temperature)\n",
    "        tree_policy = tree_policy / np.sum(tree_policy)\n",
    "        if self.exploit:\n",
    "            action = np.argmax(tree_policy)\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(node.action_space_size), p=tree_policy)\n",
    "        return tree_policy, action, node.children[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_v0(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(CNN_v0, self).__init__()\n",
    "        \n",
    "        self.convblock1 = nn.Sequential(\n",
    "            nn.Conv1d(5, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.convblock2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.convblock3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv1d(128, 50, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm1d(50), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(50 * 249, action_dim) # 4 * action_dim\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv1d(128, 50, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm1d(50), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(50 * 249, 128), \n",
    "            nn.Linear(128, 1), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        \n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = data_module.x_test[1].numpy()\n",
    "seqgame = SeqGame(sequence, standard_cnn)\n",
    "\n",
    "\n",
    "mcts_config = {\n",
    "    \"puct_coefficient\": 2.0,\n",
    "    \"num_simulations\": 10000,\n",
    "    \"temperature\": 1.5,\n",
    "    \"dirichlet_epsilon\": 0.25,\n",
    "    \"dirichlet_noise\": 0.03,\n",
    "    \"argmax_tree_policy\": False,\n",
    "    \"add_dirichlet_noise\": True,\n",
    "}\n",
    "\n",
    "model = CNN_v0(seqgame.action_size)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(model, mcts_config)\n",
    "\n",
    "# Get the initial sequence and create the root node\n",
    "initial_sequence = seqgame.get_seq()\n",
    "    \n",
    "root_node = Node(\n",
    "    state=initial_sequence,\n",
    "    reward=0,\n",
    "    done=False,\n",
    "    action=None,\n",
    "    parent=RootParentNode(env=seqgame),\n",
    "    mcts=mcts, \n",
    "    level=0, \n",
    "    tile_ranges_done=[]\n",
    ")\n",
    "\n",
    "while not root_node.done:  # Loop until the root node indicates the game is done\n",
    "    print(\"Current sequence:\", convert_elements(root_node.state[-1,:]))\n",
    "    valid_moves = root_node.valid_actions\n",
    "    print(\"Valid moves:\", [i for i in range(seqgame.action_size) if valid_moves[i] == 1])\n",
    "\n",
    "    # Perform simulations and select an action using MCTS\n",
    "    mcts_probs, action, next_node = mcts.compute_action(root_node)\n",
    "    print(\"MCTS probabilities:\", mcts_probs)\n",
    "    print(\"Selected action:\", action)\n",
    "    \n",
    "\n",
    "    if valid_moves[action] == 0:\n",
    "        print(\"Invalid action, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(root_node.tile_ranges_done)\n",
    "    print(root_node.reward)\n",
    "\n",
    "    root_node = next_node\n",
    "\n",
    "print(\"Game ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaDNA:\n",
    "    def __init__(self, model, optimizer, env, args, mcts_config = {\n",
    "        \"puct_coefficient\": 2.0,\n",
    "        \"num_simulations\": 10000,\n",
    "        \"temperature\": 1.5,\n",
    "        \"dirichlet_epsilon\": 0.25,\n",
    "        \"dirichlet_noise\": 0.03,\n",
    "        \"argmax_tree_policy\": False,\n",
    "        \"add_dirichlet_noise\": True,}\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(model, mcts_config)\n",
    "        \n",
    "        self.initial_sequence = env.get_seq()\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        state = self.env.get_seq()\n",
    "\n",
    "        root_node = Node(\n",
    "            state=self.initial_sequence,\n",
    "            reward=0,\n",
    "            done=False,\n",
    "            action=None,\n",
    "            parent=RootParentNode(env=seqgame),\n",
    "            mcts=self.mcts, \n",
    "            level=0, \n",
    "            tile_ranges_done=[]\n",
    "        )\n",
    "\n",
    "        while True:  # Loop until the root node indicates the game is done\n",
    "            valid_moves = root_node.valid_actions\n",
    "            mcts_probs, action, next_node = self.mcts.compute_action(root_node)\n",
    "            \n",
    "            memory.append((root_node.state, mcts_probs, next_node.reward))\n",
    "\n",
    "            if valid_moves[action] == 0:\n",
    "                print(\"Invalid action, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            if next_node.done:\n",
    "                return memory\n",
    "\n",
    "            root_node = next_node\n",
    "    \n",
    "    def train(self, memory):\n",
    "        np.random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            priors = nn.Softmax(dim=-1)(out_policy)\n",
    "            \n",
    "            policy_loss = torch.mean(\n",
    "                -torch.sum(policy_targets * torch.log(priors), dim=-1)\n",
    "            )\n",
    "            value_loss = torch.mean(torch.pow(value_targets - out_value, 2))\n",
    "            \n",
    "            total_loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            return total_loss\n",
    "        \n",
    "    def learn(self):\n",
    "        for iteration in tqdm(range(self.args['num_iterations'])):\n",
    "            print(f\"Iteration {iteration}:\")\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in tqdm(range(self.args['num_selfPlay_iterations'])):\n",
    "                print(f\"SelfPlay Iteration {selfPlay_iteration}\")\n",
    "                memory += self.selfPlay()\n",
    "            \n",
    "            self.model.train()\n",
    "            for epoch in tqdm(range(self.args['num_epochs'])):\n",
    "                print(f\"Training Epoch {epoch}\")\n",
    "                loss = self.train(memory)\n",
    "                print(f\"Total Loss: {loss}\")\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.97s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 134.91it/s]\n",
      "  5%|▌         | 1/20 [00:05<01:53,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 5.298501014709473\n",
      "Training Epoch 1\n",
      "Total Loss: 22.32219696044922\n",
      "Training Epoch 2\n",
      "Total Loss: 28.530973434448242\n",
      "Training Epoch 3\n",
      "Total Loss: 3.477583885192871\n",
      "Iteration 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:17<00:00,  8.86s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 138.49it/s]\n",
      " 10%|█         | 2/20 [00:23<03:52, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 22.160442352294922\n",
      "Training Epoch 1\n",
      "Total Loss: 22.214080810546875\n",
      "Training Epoch 2\n",
      "Total Loss: 21.115890502929688\n",
      "Training Epoch 3\n",
      "Total Loss: 20.44860076904297\n",
      "Iteration 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:17<00:00,  8.88s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 131.88it/s]\n",
      " 15%|█▌        | 3/20 [00:41<04:17, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 18.718368530273438\n",
      "Training Epoch 1\n",
      "Total Loss: 11.52180004119873\n",
      "Training Epoch 2\n",
      "Total Loss: 14.892873764038086\n",
      "Training Epoch 3\n",
      "Total Loss: 16.590232849121094\n",
      "Iteration 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:26<00:00, 13.14s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 118.77it/s]\n",
      " 20%|██        | 4/20 [01:07<05:13, 19.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 11.645698547363281\n",
      "Training Epoch 1\n",
      "Total Loss: 11.100095748901367\n",
      "Training Epoch 2\n",
      "Total Loss: 12.235994338989258\n",
      "Training Epoch 3\n",
      "Total Loss: 16.988523483276367\n",
      "Iteration 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:24<00:00, 12.23s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 142.80it/s]\n",
      " 25%|██▌       | 5/20 [01:32<05:20, 21.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 9.83155345916748\n",
      "Training Epoch 1\n",
      "Total Loss: 8.710453033447266\n",
      "Training Epoch 2\n",
      "Total Loss: 5.074153423309326\n",
      "Training Epoch 3\n",
      "Total Loss: 7.747463703155518\n",
      "Iteration 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:26<00:00, 13.20s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 131.61it/s]\n",
      " 30%|███       | 6/20 [01:58<05:23, 23.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 6.70822286605835\n",
      "Training Epoch 1\n",
      "Total Loss: 7.176239490509033\n",
      "Training Epoch 2\n",
      "Total Loss: 3.908320426940918\n",
      "Training Epoch 3\n",
      "Total Loss: 6.363714218139648\n",
      "Iteration 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:27<00:00, 13.54s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 133.70it/s]\n",
      " 35%|███▌      | 7/20 [02:26<05:17, 24.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 3.6277413368225098\n",
      "Training Epoch 1\n",
      "Total Loss: 7.015326499938965\n",
      "Training Epoch 2\n",
      "Total Loss: 4.756281852722168\n",
      "Training Epoch 3\n",
      "Total Loss: 3.5805766582489014\n",
      "Iteration 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:33<00:00, 16.76s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 128.31it/s]\n",
      " 40%|████      | 8/20 [02:59<05:27, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 2.1308603286743164\n",
      "Training Epoch 1\n",
      "Total Loss: 3.171949863433838\n",
      "Training Epoch 2\n",
      "Total Loss: 9.619722366333008\n",
      "Training Epoch 3\n",
      "Total Loss: 4.922009468078613\n",
      "Iteration 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.43s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 130.24it/s]\n",
      " 45%|████▌     | 9/20 [03:16<04:24, 24.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 13.017902374267578\n",
      "Training Epoch 1\n",
      "Total Loss: 10.377541542053223\n",
      "Training Epoch 2\n",
      "Total Loss: 3.364511728286743\n",
      "Training Epoch 3\n",
      "Total Loss: 3.805150032043457\n",
      "Iteration 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:35<00:00, 17.51s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 132.21it/s]\n",
      " 50%|█████     | 10/20 [03:51<04:34, 27.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 1.742547869682312\n",
      "Training Epoch 1\n",
      "Total Loss: 2.7243292331695557\n",
      "Training Epoch 2\n",
      "Total Loss: 2.248629331588745\n",
      "Training Epoch 3\n",
      "Total Loss: 2.0029866695404053\n",
      "Iteration 10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:21<00:00, 10.50s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 128.21it/s]\n",
      " 55%|█████▌    | 11/20 [04:12<03:49, 25.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 2.198237180709839\n",
      "Training Epoch 1\n",
      "Total Loss: 0.6289008259773254\n",
      "Training Epoch 2\n",
      "Total Loss: 1.0931742191314697\n",
      "Training Epoch 3\n",
      "Total Loss: 4.035010814666748\n",
      "Iteration 11:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:25<00:00, 12.98s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 134.66it/s]\n",
      " 60%|██████    | 12/20 [04:38<03:25, 25.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 10.132330894470215\n",
      "Training Epoch 1\n",
      "Total Loss: 9.091532707214355\n",
      "Training Epoch 2\n",
      "Total Loss: 1.5453221797943115\n",
      "Training Epoch 3\n",
      "Total Loss: 3.4052395820617676\n",
      "Iteration 12:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.07s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 136.21it/s]\n",
      " 65%|██████▌   | 13/20 [05:06<03:04, 26.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 1.5085400342941284\n",
      "Training Epoch 1\n",
      "Total Loss: 1.0531786680221558\n",
      "Training Epoch 2\n",
      "Total Loss: 11.770639419555664\n",
      "Training Epoch 3\n",
      "Total Loss: 3.066715717315674\n",
      "Iteration 13:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:27<00:00, 13.62s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 127.75it/s]\n",
      " 70%|███████   | 14/20 [05:34<02:40, 26.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 5.6675920486450195\n",
      "Training Epoch 1\n",
      "Total Loss: 1.8001728057861328\n",
      "Training Epoch 2\n",
      "Total Loss: 4.00806188583374\n",
      "Training Epoch 3\n",
      "Total Loss: 7.66372013092041\n",
      "Iteration 14:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:22<00:00, 11.17s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 123.84it/s]\n",
      " 75%|███████▌  | 15/20 [05:56<02:06, 25.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 0.7887231111526489\n",
      "Training Epoch 1\n",
      "Total Loss: 17.20461654663086\n",
      "Training Epoch 2\n",
      "Total Loss: 1.1866786479949951\n",
      "Training Epoch 3\n",
      "Total Loss: 2.547724962234497\n",
      "Iteration 15:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.01s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 131.93it/s]\n",
      " 80%|████████  | 16/20 [06:24<01:44, 26.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 1.1030526161193848\n",
      "Training Epoch 1\n",
      "Total Loss: 8.244341850280762\n",
      "Training Epoch 2\n",
      "Total Loss: 4.607093334197998\n",
      "Training Epoch 3\n",
      "Total Loss: 1.0634911060333252\n",
      "Iteration 16:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:30<00:00, 15.41s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 136.12it/s]\n",
      " 85%|████████▌ | 17/20 [06:55<01:22, 27.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 2.801349401473999\n",
      "Training Epoch 1\n",
      "Total Loss: 4.642921447753906\n",
      "Training Epoch 2\n",
      "Total Loss: 1.8931376934051514\n",
      "Training Epoch 3\n",
      "Total Loss: 5.27190637588501\n",
      "Iteration 17:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:27<00:00, 14.00s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 118.71it/s]\n",
      " 90%|█████████ | 18/20 [07:23<00:55, 27.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 2.2421681880950928\n",
      "Training Epoch 1\n",
      "Total Loss: 1.308841347694397\n",
      "Training Epoch 2\n",
      "Total Loss: 8.206446647644043\n",
      "Training Epoch 3\n",
      "Total Loss: 0.6455740928649902\n",
      "Iteration 18:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.78s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 135.31it/s]\n",
      " 95%|█████████▌| 19/20 [07:43<00:25, 25.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 3.396185874938965\n",
      "Training Epoch 1\n",
      "Total Loss: 0.6861117482185364\n",
      "Training Epoch 2\n",
      "Total Loss: 1.7199504375457764\n",
      "Training Epoch 3\n",
      "Total Loss: 0.9397327303886414\n",
      "Iteration 19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfPlay Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.61s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 135.75it/s]\n",
      "100%|██████████| 20/20 [08:02<00:00, 24.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "Total Loss: 2.9753501415252686\n",
      "Training Epoch 1\n",
      "Total Loss: 14.490436553955078\n",
      "Training Epoch 2\n",
      "Total Loss: 0.8867294192314148\n",
      "Training Epoch 3\n",
      "Total Loss: 16.788740158081055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sequence = data_module.x_test[1].numpy()\n",
    "seqgame = SeqGame(sequence, standard_cnn)\n",
    "\n",
    "model = CNN_v0(seqgame.action_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'num_iterations': 20, # 3 \n",
    "    'num_selfPlay_iterations': 2, # 500\n",
    "    'num_epochs': 4, \n",
    "    'batch_size': 2, # 64\n",
    "}\n",
    "\n",
    "mcts_config = {\n",
    "    \"puct_coefficient\": 2.0,\n",
    "    \"num_simulations\": 1000,\n",
    "    \"temperature\": 1.5,\n",
    "    \"dirichlet_epsilon\": 0.25,\n",
    "    \"dirichlet_noise\": 0.03,\n",
    "    \"argmax_tree_policy\": False,\n",
    "    \"add_dirichlet_noise\": True,\n",
    "}\n",
    "\n",
    "alphadna = AlphaDNA(model, optimizer, seqgame, args, mcts_config)\n",
    "alphadna.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
