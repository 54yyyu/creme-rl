{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from tqdm import tqdm\n",
    "import copy, math\n",
    "import collections\n",
    "\n",
    "from cremerl import utils, model_zoo, shuffle\n",
    "\n",
    "import shuffle_test\n",
    "\n",
    "#import gymnasium as gym\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set the logging level to WARNING\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = 'DeepSTARR'\n",
    "\n",
    "# load data\n",
    "data_path = '../../data/'\n",
    "filepath = os.path.join(data_path, expt_name+'_data.h5')\n",
    "data_module = utils.H5DataModule(filepath, batch_size=100, lower_case=False, transpose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyu/.conda/envs/tf_2/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "2023-08-11 15:06:38.700069: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-11 15:06:39.228547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/yiyu/.conda/envs/tf_2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3cd5c5f66c4372b97603a6817c45fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepstarr2 = model_zoo.deepstarr(2)\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer_dict = utils.configure_optimizer(deepstarr2, lr=0.001, weight_decay=1e-6, decay_factor=0.1, patience=5, monitor='val_loss')\n",
    "standard_cnn = model_zoo.DeepSTARR(deepstarr2,\n",
    "                                  criterion=loss,\n",
    "                                  optimizer=optimizer_dict)\n",
    "\n",
    "# load checkpoint for model with best validation performance\n",
    "standard_cnn = utils.load_model_from_checkpoint(standard_cnn, 'DeepSTARR_standard.ckpt')\n",
    "\n",
    "# evaluate best model\n",
    "pred = utils.get_predictions(standard_cnn, data_module.x_test[np.newaxis,100], batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swap_greedy(x, x_mut, tile_ranges):\n",
    "    ori = x.copy()\n",
    "    mut = x_mut.copy()\n",
    "    for tile_range in tile_ranges:\n",
    "        ori[:, tile_range[0]:tile_range[1]] = x_mut[:, tile_range[0]:tile_range[1]]\n",
    "        mut[:, tile_range[0]:tile_range[1]] = x[:, tile_range[0]:tile_range[1]]\n",
    "\n",
    "    return ori, mut\n",
    "\n",
    "def get_score(pred):\n",
    "    score1 = pred[0] - pred[2]\n",
    "    score2 = pred[3] - pred[1]\n",
    "    return (score1+score2)[0], score1+score2\n",
    "\n",
    "def generate_tile_ranges(sequence_length, window_size, stride):\n",
    "    ranges = []\n",
    "    start = np.arange(0, sequence_length - window_size + stride, stride)\n",
    "\n",
    "    for s in start:\n",
    "        e = min(s + window_size, sequence_length)\n",
    "        ranges.append([s, e])\n",
    "\n",
    "    if start[-1] + window_size - stride < sequence_length:  # Adjust the last range\n",
    "        ranges[-1][1] = sequence_length\n",
    "\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, tile_range, tile_ranges_ori, trials):\n",
    "    test_batch = []\n",
    "    for i in range(trials):\n",
    "        test_batch.append(x)\n",
    "        x_mut = shuffle.dinuc_shuffle(x.copy())\n",
    "        test_batch.append(x_mut)\n",
    "\n",
    "        ori = x.copy()\n",
    "        mut = x_mut.copy()\n",
    "        \n",
    "        ori, mut = get_swap_greedy(ori, mut, tile_ranges_ori)\n",
    "        \n",
    "        ori[:, tile_range[0]:tile_range[1]] = x_mut[:, tile_range[0]:tile_range[1]]\n",
    "        mut[:, tile_range[0]:tile_range[1]] = x[:, tile_range[0]:tile_range[1]]\n",
    "        \n",
    "        test_batch.append(ori)\n",
    "        test_batch.append(mut)\n",
    "\n",
    "    #print(np.array(test_batch).shape)\n",
    "    return np.array(test_batch)\n",
    "\n",
    "\n",
    "def get_batch_score(pred, trials):\n",
    "\n",
    "    score = []\n",
    "    score_sep = []\n",
    "    for i in range(0, pred.shape[0], 2):\n",
    "        # print(f\"Viewing number {i}\")\n",
    "        score1 = pred[0] - pred[i]\n",
    "        score2 = pred[i+1] - pred[1]\n",
    "        score.append((np.sum((score1, score2)[0])).tolist()) #np.sum(score1+score2, keepdims=True)\n",
    "        score_sep.append((score1+score2).tolist())\n",
    "        \n",
    "    # print(score)\n",
    "        \n",
    "    final = np.sum(np.array(score), axis=0)/trials\n",
    "\n",
    "    #max_ind = np.argmax(final)\n",
    "    #block_ind = np.argmax(np.array(score)[:, max_ind])\n",
    "    #print(np.array(total_score)[:, max_ind])\n",
    "    total_score_sep = np.sum(np.array(score_sep), axis=0)/trials\n",
    "\n",
    "    #print(np.max(score))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_sequence(one_hot_sequence):\n",
    "    A, L = one_hot_sequence.shape\n",
    "\n",
    "    # Create an all-ones row\n",
    "    ones_row = np.zeros(L)\n",
    "\n",
    "    # Add the all-ones row to the original sequence\n",
    "    new_sequence = np.vstack((one_hot_sequence, ones_row))\n",
    "\n",
    "    return np.array(new_sequence, dtype='float32')\n",
    "\n",
    "def taking_action(sequence_with_ones, tile_range):\n",
    "    start_idx, end_idx = tile_range\n",
    "\n",
    "    # Ensure the start_idx and end_idx are within valid bounds\n",
    "    #if start_idx < 0 or start_idx >= sequence_with_ones.shape[1] or end_idx < 0 or end_idx >= sequence_with_ones.shape[1]:\n",
    "    #    raise ValueError(\"Invalid tile range indices.\")\n",
    "\n",
    "    # Copy the input sequence to avoid modifying the original sequence\n",
    "    modified_sequence = sequence_with_ones.copy()\n",
    "\n",
    "    # Modify the last row within the specified tile range\n",
    "    modified_sequence[-1, start_idx:end_idx] = 1\n",
    "\n",
    "    return np.array(modified_sequence, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_elements(input_list):\n",
    "    input_list = input_list.tolist()\n",
    "    num_columns = 5  # Number of elements to process in each group\n",
    "\n",
    "    # Calculate the number of elements needed to pad the list\n",
    "    padding_length = num_columns - (len(input_list) % num_columns)\n",
    "    last_value = input_list[-1]\n",
    "    padded_list = input_list + [last_value] * padding_length\n",
    "\n",
    "    # Convert the padded list to a NumPy array for efficient operations\n",
    "    input_array = np.array(padded_list)\n",
    "    reshaped_array = input_array.reshape(-1, num_columns)\n",
    "\n",
    "    # Check if each row has the same value (all 0s or all 1s)\n",
    "    row_all_zeros = np.all(reshaped_array == 0, axis=1)\n",
    "    row_all_ones = np.all(reshaped_array == 1, axis=1)\n",
    "\n",
    "    # Replace all 0s with 0 and all 1s with 1 in the result array\n",
    "    output_array = np.where(row_all_zeros, 0, np.where(row_all_ones, 1, reshaped_array[:, 0]))\n",
    "\n",
    "    # Flatten the result array to get the final output list\n",
    "    output_list = output_array.flatten()\n",
    "\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqGame:\n",
    "    def __init__(self, sequence, model_func):\n",
    "        self.seq = sequence\n",
    "        self.ori_seq = sequence.copy()\n",
    "        self.tile_ranges = generate_tile_ranges(sequence.shape[1], 5, 5)\n",
    "        # self.tile_ranges_done = []\n",
    "        self.levels = 20\n",
    "        self.num_trials = 10\n",
    "        self.action_size = 50\n",
    "        \n",
    "        self.prev_score = -float(\"inf\")\n",
    "        self.current_score = 0\n",
    "        \n",
    "        self.trainer = pl.Trainer(accelerator='gpu', devices='1', logger=None, enable_progress_bar=False)\n",
    "        self.model = model_func\n",
    "        \n",
    "        if self.seq.shape[0]!=5:\n",
    "            self.seq = extend_sequence(self.seq)\n",
    "            self.ori_seq = extend_sequence(self.ori_seq)\n",
    "        \n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        self.seq = self.ori_seq.copy()\n",
    "        self.tile_ranges = generate_tile_ranges(self.seq.shape[1], 5, 5)\n",
    "        self.tile_ranges_done = []\n",
    "        self.current_level = 0\n",
    "        \n",
    "        return self.seq\n",
    "    \n",
    "    \n",
    "    def get_next_state(self, action, tile_ranges_done):\n",
    "        self.prev_score = self.current_score\n",
    "        # self.current_level += 1\n",
    "        \n",
    "        self.seq = taking_action(self.seq, self.tile_ranges[action])\n",
    "        \n",
    "        batch = get_batch(self.seq[:4, :], self.tile_ranges[action], tile_ranges_done, self.num_trials)\n",
    "        dataloader = torch.utils.data.DataLoader(batch, batch_size=100, shuffle=False)\n",
    "        pred = np.concatenate(self.trainer.predict(self.model, dataloaders=dataloader))\n",
    "        \n",
    "        self.current_score = np.tanh(1 * get_batch_score(pred, self.num_trials)) #ADDED TANH\n",
    "        \n",
    "        return self.seq\n",
    "    \n",
    "    def get_valid_moves(self):\n",
    "        return (convert_elements(self.seq[-1, :]) == 0).astype(np.uint8)\n",
    "    \n",
    "    def terminate(self, level, current_score, parent_score): #state\n",
    "        # if self.current_level >= self.levels:\n",
    "        #     return True\n",
    "        # if self.current_score < self.prev_score:\n",
    "        #     return True\n",
    "        \n",
    "        if level >= self.levels:\n",
    "            return True\n",
    "        if current_score < parent_score:\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "    \n",
    "    def set_seq(self, seq):\n",
    "        self.seq = seq\n",
    "    \n",
    "    def get_seq(self):\n",
    "        return self.seq.copy()\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, action, state, done, reward, mcts, level, tile_ranges_done, parent=None):\n",
    "        self.env = parent.env\n",
    "        self.action = action\n",
    "        \n",
    "        self.is_expanded = False\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        \n",
    "        self.action_space_size = self.env.action_size\n",
    "        self.child_total_value = np.zeros(\n",
    "            [self.action_space_size], dtype=np.float32\n",
    "        ) # Q\n",
    "        self.child_priors = np.zeros([self.action_space_size], dtype=np.float32) # P\n",
    "        self.child_number_visits = np.zeros(\n",
    "            [self.action_space_size], dtype=np.float32\n",
    "        ) # N\n",
    "        self.valid_actions = (convert_elements(state[-1, :]) == 0).astype(np.uint8)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "        self.state = state\n",
    "        self.level = level\n",
    "        \n",
    "        self.tile_ranges_done = tile_ranges_done\n",
    "        \n",
    "        self.mcts = mcts\n",
    "    \n",
    "    @property\n",
    "    def number_visits(self):\n",
    "        return self.parent.child_number_visits[self.action]\n",
    "    \n",
    "    @number_visits.setter\n",
    "    def number_visits(self, value):\n",
    "        self.parent.child_number_visits[self.action] = value\n",
    "        \n",
    "    @property\n",
    "    def total_value(self):\n",
    "        return self.parent.child_total_value[self.action]\n",
    "    \n",
    "    @total_value.setter\n",
    "    def total_value(self, value):\n",
    "        self.parent.child_total_value[self.action] = value\n",
    "        \n",
    "    def child_Q(self):\n",
    "        return self.child_total_value / (1 + self.child_number_visits)\n",
    "    \n",
    "    def child_U(self):\n",
    "        return (\n",
    "            math.sqrt(self.number_visits)\n",
    "            * self.child_priors\n",
    "            / (1 + self.child_number_visits)\n",
    "        )\n",
    "    \n",
    "    def best_action(self):\n",
    "        child_score = self.child_Q() + self.mcts.c_puct * self.child_U()\n",
    "        masked_child_score = child_score\n",
    "        # masked_child_score[~self.valid_actions] = -np.inf\n",
    "        masked_child_score = masked_child_score * self.valid_actions\n",
    "        return np.argmax(masked_child_score)\n",
    "    \n",
    "    def select(self):\n",
    "        current_node = self\n",
    "        while current_node.is_expanded:\n",
    "            best_action = current_node.best_action()\n",
    "            current_node = current_node.get_child(best_action)\n",
    "        return current_node\n",
    "    \n",
    "    def expand(self, child_priors):\n",
    "        self.is_expanded = True\n",
    "        self.child_priors = child_priors\n",
    "        \n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "        self.valid_actions = (convert_elements(state[-1, :]) == 0).astype(np.uint8)\n",
    "    \n",
    "    def get_child(self, action):\n",
    "        if action not in self.children:\n",
    "\n",
    "            self.env.set_seq(self.state.copy())\n",
    "            next_state = self.env.get_next_state(action, self.tile_ranges_done)\n",
    "            # self.tile_ranges_done.append(self.tile_ranges.pop(action))\n",
    "            new_tile_ranges_done = copy.deepcopy(self.tile_ranges_done)\n",
    "            # print(new_tile_ranges_done)\n",
    "            new_tile_ranges_done.append(self.env.tile_ranges[action])\n",
    "            # swap tile_ranges\n",
    "            reward = self.env.get_score()\n",
    "            terminated = self.env.terminate(self.level, reward, self.parent.reward)\n",
    "            self.children[action] = Node(\n",
    "                state=next_state, \n",
    "                action=action, \n",
    "                parent=self, \n",
    "                reward=reward,\n",
    "                done=terminated,\n",
    "                mcts=self.mcts, \n",
    "                level=self.level+1, \n",
    "                tile_ranges_done=new_tile_ranges_done\n",
    "            )\n",
    "        return self.children[action]\n",
    "    \n",
    "    def backup(self, value):\n",
    "        current = self\n",
    "        while current.parent is not None:\n",
    "            current.number_visits += 1\n",
    "            current.total_value += value\n",
    "            current = current.parent\n",
    "\n",
    "class RootParentNode:\n",
    "    def __init__(self, env):\n",
    "        self.parent = None\n",
    "        self.child_total_value = collections.defaultdict(float)\n",
    "        self.child_number_visits = collections.defaultdict(float)\n",
    "        self.env = env\n",
    "        self.reward = -np.inf\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, model, mcts_param):\n",
    "        self.model = model\n",
    "        self.temperature = mcts_param[\"temperature\"]\n",
    "        self.dir_epsilon = mcts_param[\"dirichlet_epsilon\"]\n",
    "        self.dir_noise = mcts_param[\"dirichlet_noise\"]\n",
    "        self.num_sims = mcts_param[\"num_simulations\"]\n",
    "        self.exploit = mcts_param[\"argmax_tree_policy\"]\n",
    "        self.add_dirichlet_noise = mcts_param[\"add_dirichlet_noise\"]\n",
    "        self.c_puct = mcts_param[\"puct_coefficient\"]\n",
    "    \n",
    "    def compute_action(self, node):\n",
    "        for _ in range(self.num_sims):\n",
    "            leaf = node.select()\n",
    "            if leaf.done:\n",
    "                value = leaf.reward\n",
    "            else:\n",
    "                child_priors, value = self.model(torch.tensor(leaf.state).unsqueeze(0))\n",
    "                child_priors = torch.softmax(child_priors, axis=1).squeeze(0).cpu().detach().numpy()\n",
    "                if self.add_dirichlet_noise:\n",
    "                    child_priors = (1 - self.dir_epsilon) * child_priors\n",
    "                    child_priors += self.dir_epsilon * np.random.dirichlet(\n",
    "                        [self.dir_noise] * child_priors.size\n",
    "                    )\n",
    "                \n",
    "                leaf.expand(child_priors)\n",
    "            leaf.backup(value)\n",
    "            \n",
    "        tree_policy = node.child_number_visits / node.number_visits\n",
    "        tree_policy = tree_policy / np.max(tree_policy)\n",
    "        tree_policy = np.power(tree_policy, self.temperature)\n",
    "        tree_policy = tree_policy / np.sum(tree_policy)\n",
    "        if self.exploit:\n",
    "            action = np.argmax(tree_policy)\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(node.action_space_size), p=tree_policy)\n",
    "        return tree_policy, action, node.children[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_v0(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(CNN_v0, self).__init__()\n",
    "        \n",
    "        self.convblock1 = nn.Sequential(\n",
    "            nn.Conv1d(5, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.convblock2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.convblock3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv1d(128, 50, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm1d(50), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(50 * 249, action_dim) # 4 * action_dim\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv1d(128, 50, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm1d(50), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(50 * 249, 128), \n",
    "            nn.Linear(128, 1), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        \n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = data_module.x_test[1].numpy()\n",
    "seqgame = SeqGame(sequence, standard_cnn)\n",
    "\n",
    "\n",
    "mcts_config = {\n",
    "    \"puct_coefficient\": 2.0,\n",
    "    \"num_simulations\": 10000,\n",
    "    \"temperature\": 1.5,\n",
    "    \"dirichlet_epsilon\": 0.25,\n",
    "    \"dirichlet_noise\": 0.03,\n",
    "    \"argmax_tree_policy\": False,\n",
    "    \"add_dirichlet_noise\": True,\n",
    "}\n",
    "\n",
    "model = CNN_v0(seqgame.action_size)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(model, mcts_config)\n",
    "state = seqgame.get_initial_state()\n",
    "tree_node = None\n",
    "while True:\n",
    "    print(state)\n",
    "    valid_moves = seqgame.get_valid_moves()\n",
    "    print(\"Valid moves\", [i for i in range(seqgame.action_size) if valid_moves[i]==1])\n",
    "    #action = int(input(\"Take action: \"))\n",
    "    #action = np.random.randint(0, 50)\n",
    "    if tree_node == None:\n",
    "        tree_node = Node(\n",
    "            state=seqgame.get_seq(), \n",
    "            reward=0, \n",
    "            done=False,\n",
    "            action=None,\n",
    "            parent=RootParentNode(env=seqgame),\n",
    "            mcts=mcts, \n",
    "            level=0, \n",
    "            tile_ranges=generate_tile_ranges(sequence.shape[1], 5, 5), \n",
    "            tile_ranges_done=[]\n",
    "        )\n",
    "    mcts_probs, action, tree_node = mcts.compute_action(tree_node)\n",
    "    print(mcts_probs)\n",
    "    #action = np.argmax(mcts_probs)\n",
    "    print(f\"This is the action: {action}\")\n",
    "    \n",
    "    if valid_moves[action] == 0:\n",
    "        print(\"Invalid action\")\n",
    "        continue\n",
    "    \n",
    "    state = seqgame.get_next_state(action)\n",
    "    print(f\"The current level is: {seqgame.current_level}\")\n",
    "    \n",
    "    print(seqgame.get_score())\n",
    "    #is_terminal = seqgame.terminate()\n",
    "    is_terminal = tree_node.done\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(\"Game ended\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = data_module.x_test[1].numpy()\n",
    "seqgame = SeqGame(sequence, standard_cnn)\n",
    "\n",
    "\n",
    "mcts_config = {\n",
    "    \"puct_coefficient\": 2.0,\n",
    "    \"num_simulations\": 10000,\n",
    "    \"temperature\": 1.5,\n",
    "    \"dirichlet_epsilon\": 0.25,\n",
    "    \"dirichlet_noise\": 0.03,\n",
    "    \"argmax_tree_policy\": False,\n",
    "    \"add_dirichlet_noise\": True,\n",
    "}\n",
    "\n",
    "model = CNN_v0(seqgame.action_size)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(model, mcts_config)\n",
    "\n",
    "# Get the initial sequence and create the root node\n",
    "initial_sequence = seqgame.get_seq()\n",
    "tile_ranges=generate_tile_ranges(sequence.shape[1], 5, 5)\n",
    "tile_ranges_done=[]\n",
    "prev_score = 0\n",
    "test_sequence = seqgame.get_seq()\n",
    "    \n",
    "root_node = Node(\n",
    "    state=initial_sequence,\n",
    "    reward=0,\n",
    "    done=False,\n",
    "    action=None,\n",
    "    parent=RootParentNode(env=seqgame),\n",
    "    mcts=mcts, \n",
    "    level=0, \n",
    "    tile_ranges_done=[]\n",
    ")\n",
    "\n",
    "while not root_node.done:  # Loop until the root node indicates the game is done\n",
    "    print(\"Current sequence:\", convert_elements(root_node.state[-1,:]))\n",
    "    valid_moves = root_node.valid_actions\n",
    "    print(\"Valid moves:\", [i for i in range(seqgame.action_size) if valid_moves[i] == 1])\n",
    "\n",
    "    # Perform simulations and select an action using MCTS\n",
    "    mcts_probs, action, next_node = mcts.compute_action(root_node)\n",
    "    print(\"MCTS probabilities:\", mcts_probs)\n",
    "    print(\"Selected action:\", action)\n",
    "    \n",
    "    print(valid_moves)\n",
    "\n",
    "    if valid_moves[action] == 0:\n",
    "        print(\"Invalid action, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(root_node.tile_ranges_done)\n",
    "    print(root_node.reward)\n",
    "\n",
    "    # next_state = seqgame.get_next_state(action)\n",
    "    # seqgame.set_seq(test_sequence)\n",
    "    # next_state = seqgame.get_next_state(action, tile_ranges, tile_ranges_done)\n",
    "    # tile_ranges_done.append(tile_ranges[action])\n",
    "    # tile_ranges[action] = [0,0]\n",
    "    # next_reward = seqgame.get_score()\n",
    "    # print(next_reward)\n",
    "    # print(next_node.level)\n",
    "    # next_done = seqgame.terminate(next_node.level, next_reward, prev_score)  # Update termination status based on your logic\n",
    "    # prev_score = next_reward\n",
    "\n",
    "    # # Create the new child node and update the root node\n",
    "    # for i in range(root_node.action_space_size):\n",
    "    #     print(\"Children's states\")\n",
    "    #     print(convert_elements(root_node.get_child(i).state[-1,:]))\n",
    "    # next_child_node = root_node.get_child(action)\n",
    "    # print(f\"This is the chosen action's child's state: {convert_elements(root_node.get_child(action).state[-1,:])}\")\n",
    "    # next_child_node.set_state(next_state)\n",
    "    # next_child_node.reward = next_reward\n",
    "    # next_child_node.done = next_done\n",
    "    # next_child_node.is_expanded = False\n",
    "    # root_node = next_child_node\n",
    "    root_node = next_node\n",
    "\n",
    "print(\"Game ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current sequence: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [8.2636916e-06 7.2403163e-05 5.3674253e-06 2.9216562e-06 5.3674253e-06\n",
      " 5.3674253e-06 5.3674253e-06 5.3674253e-06 5.3674253e-06 5.3674253e-06\n",
      " 5.3674253e-06 5.3674253e-06 2.9216562e-06 2.9216562e-06 5.3674253e-06\n",
      " 1.1548863e-05 5.3674253e-06 9.9936849e-01 5.3674253e-06 5.3674253e-06\n",
      " 4.8417147e-05 5.3674253e-06 5.3674253e-06 5.3674253e-06 5.3674253e-06\n",
      " 2.9216562e-06 5.3674253e-06 5.3674253e-06 5.3674253e-06 5.3674253e-06\n",
      " 1.8698600e-04 5.3674253e-06 5.3674253e-06 5.3674253e-06 5.3674253e-06\n",
      " 5.3674253e-06 5.3674253e-06 5.3674253e-06 5.3674253e-06 3.2665117e-05\n",
      " 5.4109831e-05 5.3674253e-06 5.3674253e-06 5.3674253e-06 5.3674253e-06\n",
      " 5.3674253e-06 5.3674253e-06 1.1548863e-05 2.9216562e-06 2.9216562e-06]\n",
      "Selected action: 17\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[]\n",
      "0\n",
      "Current sequence: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [3.0177241e-06 4.2173979e-06 5.5439127e-06 3.0177241e-06 5.5439127e-06\n",
      " 4.2173979e-06 2.6440088e-05 3.0177241e-06 3.0177241e-06 4.2173979e-06\n",
      " 1.1928602e-05 4.2173979e-06 3.0177241e-06 3.0177241e-06 6.9861285e-06\n",
      " 8.5354131e-06 4.2173979e-06 0.0000000e+00 6.9861285e-06 2.9951194e-03\n",
      " 4.2173979e-06 3.0177241e-06 3.0177241e-06 3.0177241e-06 3.0177241e-06\n",
      " 6.9861285e-06 1.9759756e-05 3.0177241e-06 4.2173979e-06 1.1928602e-05\n",
      " 3.0177241e-06 6.9861285e-06 8.5354131e-06 5.5439127e-06 3.0177241e-06\n",
      " 3.0177241e-06 9.9674219e-01 1.5680553e-05 4.2173979e-06 3.0177241e-06\n",
      " 3.0177241e-06 3.0177241e-06 3.0177241e-06 3.0177241e-06 5.5439127e-06\n",
      " 5.5439127e-06 3.0177241e-06 3.0177241e-06 3.0177241e-06 4.2173979e-06]\n",
      "Selected action: 36\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[85, 90]]\n",
      "0.6090498267101914\n",
      "Current sequence: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [5.5770004e-05 3.5640998e-05 2.0679410e-05 3.6125441e-06 2.5321334e-04\n",
      " 3.6125441e-06 2.1521028e-03 6.4058462e-05 2.3120282e-04 7.1369223e-02\n",
      " 1.3431669e-05 3.6125441e-06 8.1742539e-05 3.1094754e-05 8.1742539e-05\n",
      " 7.2721152e-05 3.6125441e-06 0.0000000e+00 3.6125441e-06 3.6125441e-06\n",
      " 1.2466968e-03 4.0389470e-05 6.4058462e-05 1.2241137e-02 3.5640998e-05\n",
      " 3.6125441e-06 3.6125441e-06 3.6125441e-06 1.0217817e-05 1.1547894e-02\n",
      " 3.6125441e-06 3.6125441e-06 4.5331886e-05 3.3342039e-05 1.0080796e-04\n",
      " 7.3112747e-06 0.0000000e+00 6.9793001e-05 3.6125441e-06 3.1982290e-03\n",
      " 8.7809378e-01 3.6125441e-06 3.7990500e-05 3.6125441e-06 3.7990500e-05\n",
      " 7.3112747e-06 9.9128205e-03 6.4058462e-05 8.6515248e-03 4.0389470e-05]\n",
      "Selected action: 40\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[85, 90], [180, 185]]\n",
      "0.942520510746455\n",
      "Current sequence: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [8.0436971e-03 2.6170220e-03 1.4679510e-03 2.0965045e-02 2.1022828e-02\n",
      " 1.9707359e-02 1.2930859e-03 4.0848455e-03 4.5717490e-04 8.9191832e-03\n",
      " 1.4893016e-02 7.8347772e-03 6.7586345e-03 3.4022913e-03 6.5224235e-06\n",
      " 7.9390081e-03 6.0013048e-03 0.0000000e+00 5.9253392e-03 4.6146605e-03\n",
      " 1.9060327e-02 4.3385183e-03 7.2885479e-04 6.3083712e-03 3.4496544e-03\n",
      " 5.1474930e-03 6.5224235e-06 6.5224235e-06 3.2926295e-03 1.0291609e-03\n",
      " 6.5224235e-06 1.2703340e-03 3.2150359e-03 4.3318639e-04 1.4088434e-03\n",
      " 7.5248526e-03 0.0000000e+00 7.2596455e-03 2.9859922e-03 1.7500443e-03\n",
      " 0.0000000e+00 5.0393799e-03 7.5826609e-01 1.8519729e-03 8.2332874e-03\n",
      " 6.1873411e-04 2.5738454e-03 1.4208586e-04 6.9176564e-03 1.1806847e-03]\n",
      "Selected action: 42\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1]\n",
      "[[85, 90], [180, 185], [200, 205]]\n",
      "0.9971907440886443\n",
      "Current sequence: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Valid moves: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 43, 44, 45, 46, 47, 48, 49]\n",
      "MCTS probabilities: [2.10665166e-02 2.38034446e-02 3.27859335e-02 2.96648405e-02\n",
      " 1.53368533e-01 1.76309533e-02 1.57316905e-02 2.02167463e-02\n",
      " 2.54253875e-02 1.15915779e-02 1.21569578e-02 2.73376168e-03\n",
      " 5.02224080e-03 7.48850778e-03 1.79544501e-02 1.56799685e-02\n",
      " 2.07819659e-02 0.00000000e+00 2.45012925e-03 1.86387659e-03\n",
      " 1.48602081e-02 7.04847230e-03 1.73824932e-03 2.62189414e-02\n",
      " 1.11293548e-03 7.81406742e-03 1.29246309e-05 1.71347649e-03\n",
      " 2.45548949e-01 2.39465642e-03 1.29246309e-05 2.47802422e-03\n",
      " 2.75178887e-02 6.93001179e-03 1.37572698e-02 3.43179405e-02\n",
      " 0.00000000e+00 7.24738743e-03 1.06240185e-02 2.28592474e-02\n",
      " 0.00000000e+00 1.84433609e-02 0.00000000e+00 5.52544044e-03\n",
      " 1.73094198e-02 1.79544501e-02 2.87110321e-02 1.27795003e-02\n",
      " 1.69368032e-02 1.07149510e-02]\n",
      "Selected action: 28\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 0 1 1 1 1 1 1 1]\n",
      "[[85, 90], [180, 185], [200, 205], [210, 215]]\n",
      "0.9953753056507563\n",
      "Game ended.\n"
     ]
    }
   ],
   "source": [
    "sequence = data_module.x_test[1].numpy()\n",
    "seqgame = SeqGame(sequence, standard_cnn)\n",
    "\n",
    "\n",
    "mcts_config = {\n",
    "    \"puct_coefficient\": 2.0,\n",
    "    \"num_simulations\": 10000,\n",
    "    \"temperature\": 1.5,\n",
    "    \"dirichlet_epsilon\": 0.25,\n",
    "    \"dirichlet_noise\": 0.03,\n",
    "    \"argmax_tree_policy\": False,\n",
    "    \"add_dirichlet_noise\": True,\n",
    "}\n",
    "\n",
    "model = CNN_v0(seqgame.action_size)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(model, mcts_config)\n",
    "\n",
    "# Get the initial sequence and create the root node\n",
    "initial_sequence = seqgame.get_seq()\n",
    "tile_ranges=generate_tile_ranges(sequence.shape[1], 5, 5)\n",
    "tile_ranges_done=[]\n",
    "prev_score = 0\n",
    "test_sequence = seqgame.get_seq()\n",
    "    \n",
    "root_node = Node(\n",
    "    state=initial_sequence,\n",
    "    reward=0,\n",
    "    done=False,\n",
    "    action=None,\n",
    "    parent=RootParentNode(env=seqgame),\n",
    "    mcts=mcts, \n",
    "    level=0, \n",
    "    tile_ranges_done=[]\n",
    ")\n",
    "\n",
    "while not root_node.done:  # Loop until the root node indicates the game is done\n",
    "    print(\"Current sequence:\", convert_elements(root_node.state[-1,:]))\n",
    "    valid_moves = root_node.valid_actions\n",
    "    print(\"Valid moves:\", [i for i in range(seqgame.action_size) if valid_moves[i] == 1])\n",
    "\n",
    "    # Perform simulations and select an action using MCTS\n",
    "    mcts_probs, action, next_node = mcts.compute_action(root_node)\n",
    "    print(\"MCTS probabilities:\", mcts_probs)\n",
    "    print(\"Selected action:\", action)\n",
    "    \n",
    "\n",
    "    if valid_moves[action] == 0:\n",
    "        print(\"Invalid action, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(root_node.tile_ranges_done)\n",
    "    print(root_node.reward)\n",
    "\n",
    "    root_node = next_node\n",
    "\n",
    "print(\"Game ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
